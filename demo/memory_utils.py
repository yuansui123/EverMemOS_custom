"""å…±äº«å·¥å…·å‡½æ•°æ¨¡å— - ç”¨äºè®°å¿†æå–å’Œå¯¹è¯ç³»ç»Ÿ

æœ¬æ¨¡å—æä¾›å…¬å…±çš„å·¥å…·å‡½æ•°ï¼Œä¾› extract_memory.py å’Œ chat_with_memory.py å…±åŒä½¿ç”¨ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- MongoDB è¿æ¥å’Œåˆå§‹åŒ–
- Profile åŠ è½½å’Œç®¡ç†
- MemCell æŸ¥è¯¢
- æ£€ç´¢ç­–ç•¥ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰
- æ—¶é—´åºåˆ—åŒ–å·¥å…·
- æ€§èƒ½ç›‘æ§å’Œè¿›åº¦è¿½è¸ªï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
"""

import json
import os
import time
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from dataclasses import dataclass, field

from agentic_layer.vectorize_service import get_vectorize_service


import numpy as np
import requests
from motor.motor_asyncio import AsyncIOMotorClient
from beanie import init_beanie

# å¯¼å…¥é¡¹ç›®ä¸­çš„æ–‡æ¡£æ¨¡å‹
import sys

PROJECT_ROOT = Path(__file__).resolve().parents[1]
src_path = str(PROJECT_ROOT / "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨è·¯å¾„ä¸­
project_root = str(PROJECT_ROOT)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.infra_layer.adapters.out.persistence.document.memory.memcell import (
    MemCell as DocMemCell,
)
from demo.memory_config import MongoDBConfig, EmbeddingConfig


def cosine_similarity(vec1, vec2):
    """
    è®¡ç®—ä¸¤ä¸ª numpy å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

    å‚æ•°:
    vec1 (np.ndarray): ç¬¬ä¸€ä¸ªå‘é‡ã€‚
    vec2 (np.ndarray): ç¬¬äºŒä¸ªå‘é‡ã€‚

    è¿”å›:
    float: ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
    """
    # è®¡ç®—ç‚¹ç§¯
    dot_product = np.dot(vec1, vec2)

    # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ L2 èŒƒæ•°ï¼ˆå³å‘é‡çš„æ¨¡ï¼‰
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    # æ·»åŠ ä¸€ä¸ªå¾ˆå°çš„æ•° (epsilon) æ¥é˜²æ­¢é™¤ä»¥é›¶
    epsilon = 1e-8
    similarity = dot_product / (norm_vec1 * norm_vec2 + epsilon)

    return similarity


# ============================================================================
# æ€§èƒ½ç›‘æ§å·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
# ============================================================================


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡è¿½è¸ªå™¨

    ç”¨äºè¿½è¸ªå’ŒæŠ¥å‘Šæå–è¿‡ç¨‹çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š
    - LLM è°ƒç”¨æ¬¡æ•°å’Œè€—æ—¶
    - MongoDB å†™å…¥æ¬¡æ•°å’Œè€—æ—¶
    - å‘é‡åŒ–è°ƒç”¨æ¬¡æ•°å’Œè€—æ—¶
    - MemCell å’Œ Profile æ•°é‡
    """

    total_start_time: float = 0.0
    memcell_count: int = 0
    profile_count: int = 0
    llm_calls: int = 0
    llm_total_time: float = 0.0
    mongo_writes: int = 0
    mongo_total_time: float = 0.0
    embedding_calls: int = 0
    embedding_total_time: float = 0.0

    def report(self) -> None:
        """ç”Ÿæˆå¹¶æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        total_time = time.time() - self.total_start_time

        print("\n" + "=" * 80)
        print("âš¡ æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š")
        print("=" * 80)
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")

        print(f"\nğŸ“Š æå–ç»“æœ:")
        print(f"  - MemCell æ•°é‡: {self.memcell_count}")
        print(f"  - Profile æ•°é‡: {self.profile_count}")

        print(f"\nğŸ¤– LLM è°ƒç”¨:")
        print(f"  - æ€»è°ƒç”¨æ¬¡æ•°: {self.llm_calls}")
        print(f"  - æ€»è€—æ—¶: {self.llm_total_time:.2f}ç§’")
        if self.llm_calls > 0:
            print(f"  - å¹³å‡è€—æ—¶: {self.llm_total_time/self.llm_calls:.2f}ç§’/æ¬¡")

        print(f"\nğŸ’¾ MongoDB å†™å…¥:")
        print(f"  - æ€»å†™å…¥æ¬¡æ•°: {self.mongo_writes}")
        print(f"  - æ€»è€—æ—¶: {self.mongo_total_time:.2f}ç§’")
        if self.mongo_writes > 0:
            print(f"  - å¹³å‡è€—æ—¶: {self.mongo_total_time/self.mongo_writes:.3f}ç§’/æ¬¡")

        print(f"\nğŸ”¢ å‘é‡åŒ–:")
        print(f"  - æ€»è°ƒç”¨æ¬¡æ•°: {self.embedding_calls}")
        print(f"  - æ€»è€—æ—¶: {self.embedding_total_time:.2f}ç§’")
        if self.embedding_calls > 0:
            print(
                f"  - å¹³å‡è€—æ—¶: {self.embedding_total_time/self.embedding_calls:.3f}ç§’/æ¬¡"
            )

        print("=" * 80 + "\n")


class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨

    ç”¨äºæ˜¾ç¤ºå®æ—¶è¿›åº¦æ¡ã€å¤„ç†é€Ÿåº¦å’Œé¢„è®¡å®Œæˆæ—¶é—´ã€‚
    """

    def __init__(self, total: int, desc: str = "å¤„ç†ä¸­"):
        """åˆå§‹åŒ–è¿›åº¦è¿½è¸ªå™¨

        Args:
            total: æ€»ä»»åŠ¡æ•°
            desc: ä»»åŠ¡æè¿°
        """
        self.total = total
        self.current = 0
        self.desc = desc
        self.start_time = time.time()
        self.last_update = 0

    def update(self, n: int = 1) -> None:
        """æ›´æ–°è¿›åº¦

        Args:
            n: æœ¬æ¬¡å®Œæˆçš„ä»»åŠ¡æ•°
        """
        self.current += n
        current_time = time.time()

        # æ¯ 0.5 ç§’æˆ–å®Œæˆæ—¶æ›´æ–°æ˜¾ç¤º
        if current_time - self.last_update > 0.5 or self.current >= self.total:
            elapsed = current_time - self.start_time
            rate = self.current / elapsed if elapsed > 0 else 0
            eta = (self.total - self.current) / rate if rate > 0 else 0

            pct = 100.0 * self.current / self.total if self.total > 0 else 0
            bar_len = 40
            filled = int(bar_len * self.current / self.total) if self.total > 0 else 0
            bar = 'â–ˆ' * filled + '-' * (bar_len - filled)

            print(
                f'\r{self.desc}: |{bar}| {self.current}/{self.total} [{pct:.1f}%] '
                f'{rate:.1f}it/s ETA: {eta:.0f}s',
                end='',
                flush=True,
            )

            self.last_update = current_time

            if self.current >= self.total:
                print()  # å®Œæˆåæ¢è¡Œ


# ============================================================================
# æ‰¹é‡ MongoDB å†™å…¥å™¨ï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
# ============================================================================


class BatchMongoWriter:
    """æ‰¹é‡ MongoDB å†™å…¥å™¨

    ä½¿ç”¨ç¼“å†²åŒºæ‰¹é‡å†™å…¥ MongoDBï¼Œå‡å°‘æ•°æ®åº“ IO æ¬¡æ•°ï¼Œæé«˜æ€§èƒ½ã€‚
    """

    def __init__(
        self, batch_size: int = 20, perf_metrics: Optional[PerformanceMetrics] = None
    ):
        """åˆå§‹åŒ–æ‰¹é‡å†™å…¥å™¨

        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            perf_metrics: æ€§èƒ½æŒ‡æ ‡è¿½è¸ªå™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.batch_size = batch_size
        self.buffer: List = []
        self.lock = asyncio.Lock()
        self.perf_metrics = perf_metrics

    async def add(self, memcell) -> None:
        """æ·»åŠ  MemCell åˆ°ç¼“å†²åŒº

        Args:
            memcell: MemCell å¯¹è±¡
        """
        async with self.lock:
            doc = self._create_doc(memcell)
            self.buffer.append(doc)

            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶è‡ªåŠ¨åˆ·æ–°
            if len(self.buffer) >= self.batch_size:
                await self._flush()

    async def flush(self) -> None:
        """å…¬å¼€çš„åˆ·æ–°æ¥å£"""
        async with self.lock:
            await self._flush()

    async def _flush(self) -> None:
        """å†…éƒ¨åˆ·æ–°é€»è¾‘ï¼ˆæ— é”ï¼‰"""
        if not self.buffer:
            return

        start_time = time.time()
        try:
            # å¯¼å…¥æ–‡æ¡£æ¨¡å‹
            from src.infra_layer.adapters.out.persistence.document.memory.memcell import (
                MemCell as DocMemCell,
            )

            # æ‰¹é‡æ’å…¥
            await DocMemCell.insert_many(self.buffer)

            elapsed = time.time() - start_time

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            if self.perf_metrics:
                self.perf_metrics.mongo_writes += len(self.buffer)
                self.perf_metrics.mongo_total_time += elapsed

            print(
                f"[MongoDB] æ‰¹é‡å†™å…¥ {len(self.buffer)} ä¸ªMemCellï¼Œè€—æ—¶ {elapsed:.2f}ç§’"
            )
            self.buffer.clear()

        except Exception as e:
            print(f"[MongoDB] âŒ æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
            self.buffer.clear()

    def _create_doc(self, memcell):
        """åˆ›å»º MongoDB æ–‡æ¡£å¯¹è±¡

        Args:
            memcell: MemCell å¯¹è±¡

        Returns:
            DocMemCell æ–‡æ¡£å¯¹è±¡
        """
        from src.infra_layer.adapters.out.persistence.document.memory.memcell import (
            MemCell as DocMemCell,
            DataTypeEnum,
        )
        from src.common_utils.datetime_utils import (
            from_iso_format,
            get_now_with_timezone,
        )

        # è§£ææ—¶é—´æˆ³
        ts = memcell.timestamp
        if isinstance(ts, str):
            ts_dt = from_iso_format(ts)
        elif isinstance(ts, (int, float)):
            tz = get_now_with_timezone().tzinfo
            ts_dt = datetime.fromtimestamp(float(ts), tz=tz)
        else:
            ts_dt = ts or get_now_with_timezone()

        # è·å–ä¸»ç”¨æˆ· ID
        primary_user = (
            memcell.user_id_list[0]
            if getattr(memcell, 'user_id_list', None)
            else "default"
        )

        return DocMemCell(
            user_id=primary_user,
            timestamp=ts_dt,
            summary=memcell.summary or "",
            group_id=getattr(memcell, 'group_id', None),
            participants=getattr(memcell, 'participants', None),
            type=DataTypeEnum.CONVERSATION,
            subject=getattr(memcell, 'subject', None),
            keywords=getattr(memcell, 'keywords', None),
            linked_entities=getattr(memcell, 'linked_entities', None),
            episode=getattr(memcell, 'episode', None),
            semantic_memories=getattr(memcell, 'semantic_memories', None),
            extend=getattr(memcell, 'extend', None),
        )


# ============================================================================
# MongoDB ç›¸å…³å·¥å…·
# ============================================================================


async def ensure_mongo_beanie_ready(mongo_config: MongoDBConfig) -> None:
    """åˆå§‹åŒ– MongoDB å’Œ Beanie è¿æ¥

    Args:
        mongo_config: MongoDB é…ç½®å¯¹è±¡

    Raises:
        Exception: å¦‚æœè¿æ¥å¤±è´¥
    """
    # è®¾ç½®ç¯å¢ƒå˜é‡ä¾› Beanie ä½¿ç”¨
    os.environ["MONGODB_URI"] = mongo_config.uri

    # åˆ›å»º MongoDB å®¢æˆ·ç«¯å¹¶æµ‹è¯•è¿æ¥
    client = AsyncIOMotorClient(mongo_config.uri)
    try:
        await client.admin.command('ping')
        print(f"[MongoDB] âœ… è¿æ¥æˆåŠŸ: {mongo_config.database}")
    except Exception as e:
        print(f"[MongoDB] âŒ è¿æ¥å¤±è´¥: {e}")
        raise

    # åˆå§‹åŒ– Beanie æ–‡æ¡£æ¨¡å‹
    await init_beanie(
        database=client[mongo_config.database], document_models=[DocMemCell]
    )


async def query_all_groups_from_mongodb() -> List[Dict[str, Any]]:
    """æŸ¥è¯¢æ‰€æœ‰ç¾¤ç»„ ID åŠå…¶è®°å¿†æ•°é‡

    ä½¿ç”¨èšåˆç®¡é“ç»Ÿè®¡æ¯ä¸ªç¾¤ç»„çš„ MemCell æ•°é‡ã€‚

    Returns:
        ç¾¤ç»„åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"group_id": "xxx", "memcell_count": 76}, ...]
    """
    # ä½¿ç”¨èšåˆç®¡é“ç»Ÿè®¡æ¯ä¸ªç¾¤ç»„çš„è®°å¿†æ•°é‡
    pipeline = [
        {"$match": {"group_id": {"$ne": None}}},  # è¿‡æ»¤æ‰æ²¡æœ‰ group_id çš„è®°å½•
        {"$group": {"_id": "$group_id", "count": {"$sum": 1}}},
        {"$sort": {"_id": 1}},  # æŒ‰ group_id æ’åº
    ]

    # è·å– PyMongo/Motor é›†åˆè¿›è¡ŒèšåˆæŸ¥è¯¢
    # get_pymongo_collection() åœ¨ Beanie ä¸­è¿”å› Motor é›†åˆï¼ˆå¼‚æ­¥ï¼‰
    collection = DocMemCell.get_pymongo_collection()
    cursor = collection.aggregate(pipeline)
    results = await cursor.to_list(length=None)

    groups = []
    for result in results:
        groups.append({"group_id": result["_id"], "memcell_count": result["count"]})

    return groups


async def query_memcells_by_group_and_time(
    group_id: str, start_date: datetime, end_date: datetime
) -> List[DocMemCell]:
    """æŒ‰ç¾¤ç»„å’Œæ—¶é—´èŒƒå›´æŸ¥è¯¢ MemCell

    Args:
        group_id: ç¾¤ç»„ ID
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ

    Returns:
        MemCell æ–‡æ¡£å¯¹è±¡åˆ—è¡¨
    """
    memcells = (
        await DocMemCell.find(
            {"group_id": group_id, "timestamp": {"$gte": start_date, "$lt": end_date}}
        )
        .sort("timestamp")
        .to_list()
    )

    return memcells


# ============================================================================
# Profile ç›¸å…³å·¥å…·
# ============================================================================


def load_user_profiles_from_dir(output_dir: Path) -> Dict[str, Dict[str, Any]]:
    """åŠ è½½ç›®å½•ä¸­æ‰€æœ‰ç”¨æˆ·çš„ä¸ªäºº Profile

    ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰ profile_user_*.json æ–‡ä»¶ã€‚

    Args:
        output_dir: Profile æ–‡ä»¶æ‰€åœ¨ç›®å½•

    Returns:
        ç”¨æˆ· Profile å­—å…¸ï¼Œæ ¼å¼ï¼š{"user_101": {...}, "user_102": {...}, ...}
    """
    profiles = {}

    # æŸ¥æ‰¾æ‰€æœ‰ profile_user_*.json æ–‡ä»¶
    for profile_file in output_dir.glob("profile_user_*.json"):
        try:
            with profile_file.open("r", encoding="utf-8") as fp:
                profile_data = json.load(fp)

                # ä»æ–‡ä»¶åæå– user_id
                # ä¾‹å¦‚ï¼šprofile_user_101.json -> user_101
                user_id = profile_file.stem.replace("profile_", "")
                profiles[user_id] = profile_data

        except Exception as e:
            print(f"[Profile] âš ï¸ åŠ è½½å¤±è´¥ {profile_file.name}: {e}")
            continue

    return profiles


def get_user_name_from_profile(profile: Dict[str, Any]) -> Optional[str]:
    """ä» Profile ä¸­æå–ç”¨æˆ·åç§°

    Args:
        profile: Profile æ•°æ®å­—å…¸

    Returns:
        ç”¨æˆ·åç§°ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
    """
    # å°è¯•ä»ä¸åŒå­—æ®µæå–ç”¨æˆ·å
    return profile.get("user_name") or profile.get("name") or profile.get("subject")


def get_group_name_from_profiles(profiles: Dict[str, Dict]) -> Optional[str]:
    """ä» Profile ä¸­æå–ç¾¤ç»„åç§°ï¼ˆå¦‚æœæœ‰ï¼‰

    Args:
        profiles: ç”¨æˆ· Profile å­—å…¸

    Returns:
        ç¾¤ç»„åç§°ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
    """
    for profile in profiles.values():
        group_name = profile.get("group_name")
        if group_name:
            return group_name
    return None


# ============================================================================
# æ£€ç´¢ç­–ç•¥
# ============================================================================


class RetrievalStrategy:
    """æ£€ç´¢ç­–ç•¥åŸºç±»

    ç”¨äºæ‰©å±•ä¸åŒçš„æ£€ç´¢æ–¹æ³•ï¼ˆå‘é‡ç›¸ä¼¼åº¦ã€BM25ã€æ··åˆæ£€ç´¢ç­‰ï¼‰ã€‚
    """

    def __init__(self, embedding_config: EmbeddingConfig):
        """åˆå§‹åŒ–æ£€ç´¢ç­–ç•¥

        Args:
            embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
        """
        self.embedding_config = embedding_config
        self.vectorize_service = get_vectorize_service()

    async def retrieve(
        self, query: str, candidates: List[DocMemCell], top_k: int
    ) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ£€ç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            candidates: å€™é€‰ MemCell åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            æ’åºåçš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° retrieve æ–¹æ³•")


class VectorSimilarityStrategy(RetrievalStrategy):
    """åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ£€ç´¢ç­–ç•¥

    ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å¯¹å€™é€‰ MemCell è¿›è¡Œæ’åºã€‚
    """

    async def retrieve(
        self, query: str, candidates: List[DocMemCell], top_k: int
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œæ£€ç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            candidates: å€™é€‰ MemCell åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            æ’åºåçš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not candidates:
            return []

        # è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
        # q_vec = self._embed_texts_http([query])
        q_vec = await self.vectorize_service.get_embedding(query)
        # if not q_vec:
        #     print("[VectorSimilarity] âŒ æŸ¥è¯¢å‘é‡åµŒå…¥å¤±è´¥")
        #     return []

        # è·å–æ–‡æ¡£çš„åµŒå…¥å‘é‡
        # q = np.array(q_vec[0], dtype=np.float32)
        # doc_vecs = self._embed_texts_http(texts)
        doc_episode_vecs = []
        for candidate in candidates:
            try:
                doc_episode_vecs.append(candidate.extend["embedding"])
            except:
                doc_episode_vecs.append([0 for _ in range(1024)])

        if len(doc_episode_vecs) != len(candidates):
            print(
                f"[VectorSimilarity] âš ï¸ åµŒå…¥å‘é‡æ•°é‡ä¸åŒ¹é…: {len(doc_episode_vecs)} != {len(candidates)}"
            )
            return []

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        scores: List[float] = []
        for v in doc_episode_vecs:
            dv = np.array(v)

            score = cosine_similarity(q_vec, dv)
            # print(score)
            scores.append(score)

        # æ’åºå¹¶è¿”å› Top-K
        ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)[
            :top_k
        ]

        # æ„å»ºç»“æœåˆ—è¡¨
        results: List[Dict[str, Any]] = []
        for m, s in ranked:
            item = {
                "event_id": str(getattr(m, "event_id", getattr(m, "id", ""))),
                "timestamp": (
                    getattr(m, "timestamp", None).isoformat()
                    if getattr(m, "timestamp", None)
                    else None
                ),
                "group_id": getattr(m, "group_id", None),
                "subject": getattr(m, "subject", None),
                "summary": getattr(m, "summary", None),
                "episode": getattr(m, "episode", None),
                "participants": getattr(m, "participants", []),
                "score": round(s, 4),
            }
            results.append(item)

        return results

    async def retrieve_semantic(
        self, query: str, candidates: List[DocMemCell], date_query: datetime, top_k: int
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œæ£€ç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            candidates: å€™é€‰ MemCell åˆ—è¡¨
            date_query: æ—¥æœŸæŸ¥è¯¢æ¡ä»¶
            top_k: è¿”å›ç»“æœæ•°é‡
        """
        # è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
        # q_vec = self._embed_texts_http([query])
        q_vec = await self.vectorize_service.get_embedding(query)

        # è§„èŒƒåŒ– date_queryï¼Œç§»é™¤æ—¶åŒºä¿¡æ¯ä»¥ä¾¿æ¯”è¾ƒ
        # è¿™æ ·å¯ä»¥å…¼å®¹ offset-naive å’Œ offset-aware çš„ datetime
        date_query_naive = (
            date_query.replace(tzinfo=None) if date_query.tzinfo else date_query
        )

        doc_semantic_memories_vecs = []
        candidate_filtered = []
        for candidate in candidates:
            # ğŸ”¥ æ£€æŸ¥ semantic_memories æ˜¯å¦ä¸º None
            if not candidate.semantic_memories:
                continue
            
            semantic_memories_vecs = []
            for semantic_memory in candidate.semantic_memories:
                # è·å– end_time å¹¶è§„èŒƒåŒ–ä¸º naive datetime ä»¥ä¾¿æ¯”è¾ƒ
                end_time = semantic_memory['end_time']

                # å…¼å®¹å¤šç§æ•°æ®æ ¼å¼ï¼šå­—ç¬¦ä¸²æˆ–datetimeå¯¹è±¡
                if isinstance(end_time, str):
                    # å­—ç¬¦ä¸²æ ¼å¼ï¼Œè§£æä¸º datetime
                    try:
                        end_time_dt = datetime.strptime(end_time, "%Y-%m-%d")
                    except ValueError:
                        # å°è¯•è§£æå¸¦æ—¶åŒºçš„ISOæ ¼å¼
                        try:
                            end_time_dt = datetime.fromisoformat(end_time)
                            # ç§»é™¤æ—¶åŒºä¿¡æ¯
                            end_time_dt = end_time_dt.replace(tzinfo=None)
                        except ValueError:
                            # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡æ­¤è®°å¿†
                            continue
                elif isinstance(end_time, datetime):
                    # å·²ç»æ˜¯ datetime å¯¹è±¡ï¼Œç§»é™¤æ—¶åŒºä¿¡æ¯
                    end_time_dt = (
                        end_time.replace(tzinfo=None) if end_time.tzinfo else end_time
                    )
                else:
                    # ä¸æ”¯æŒçš„ç±»å‹ï¼Œè·³è¿‡
                    continue

                # æ¯”è¾ƒæ—¥æœŸï¼ˆéƒ½æ˜¯ naive datetimeï¼‰
                if end_time_dt < date_query_naive:
                    continue
                semantic_memories_vecs.append(semantic_memory["embedding"])
            if len(semantic_memories_vecs) == 0:
                continue
            else:
                doc_semantic_memories_vecs.append(semantic_memories_vecs)
                candidate_filtered.append(candidate)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        scores: List[float] = []
        for v in doc_semantic_memories_vecs:
            max_score = 0
            for semantic_memory_vec in v:
                score = cosine_similarity(q_vec, np.array(semantic_memory_vec))
                if score > max_score:
                    max_score = score
            scores.append(float(max_score))
        # æ’åºå¹¶è¿”å› Top-K
        ranked = sorted(
            zip(candidate_filtered, scores), key=lambda x: x[1], reverse=True
        )[:top_k]

        # æ„å»ºç»“æœåˆ—è¡¨
        results: List[Dict[str, Any]] = []
        for m, s in ranked:
            item = {
                "event_id": str(getattr(m, "event_id", getattr(m, "id", ""))),
                "timestamp": (
                    getattr(m, "timestamp", None).isoformat()
                    if getattr(m, "timestamp", None)
                    else None
                ),
                "group_id": getattr(m, "group_id", None),
                "subject": getattr(m, "subject", None),
                "summary": getattr(m, "summary", None),
                "episode": getattr(m, "episode", None),
                "participants": getattr(m, "participants", []),
                "score": round(s, 4),
            }
            results.append(item)

        return results

    def _embed_texts_http(self, texts: List[str]) -> List[np.ndarray]:
        """é€šè¿‡ HTTP è°ƒç”¨åµŒå…¥æœåŠ¡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []

        try:
            resp = requests.post(
                self.embedding_config.base_url,
                json={"input": texts, "model": self.embedding_config.model},
                timeout=30,  # 30ç§’è¶…æ—¶
            ).json()

            vecs = [
                np.array(item.get("embedding", []), dtype=np.float32)
                for item in resp.get("data", [])
            ]
            return vecs

        except requests.exceptions.Timeout:
            print(f"[Embedding] âŒ è¯·æ±‚è¶…æ—¶")
            return []
        except requests.exceptions.RequestException as e:
            print(f"[Embedding] âŒ è¯·æ±‚å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"[Embedding] âŒ æœªçŸ¥é”™è¯¯: {e}")
            return []


# ============================================================================
# æ—¶é—´åºåˆ—åŒ–å·¥å…·
# ============================================================================


def serialize_datetime(obj: Any) -> Any:
    """é€’å½’åºåˆ—åŒ– datetime å¯¹è±¡ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²

    Args:
        obj: è¦åºåˆ—åŒ–çš„å¯¹è±¡ï¼ˆå¯ä»¥æ˜¯ä»»æ„ç±»å‹ï¼‰

    Returns:
        åºåˆ—åŒ–åçš„å¯¹è±¡
    """
    # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›ï¼ˆé¿å…å¤„ç†å·²åºåˆ—åŒ–çš„æ—¶é—´æˆ³ï¼‰
    if isinstance(obj, str):
        return obj
    # datetime å¯¹è±¡è½¬ä¸º ISO å­—ç¬¦ä¸²
    elif isinstance(obj, datetime):
        return obj.isoformat()
    # é€’å½’å¤„ç†å­—å…¸
    elif isinstance(obj, dict):
        return {k: serialize_datetime(v) for k, v in obj.items()}
    # é€’å½’å¤„ç†åˆ—è¡¨
    elif isinstance(obj, list):
        return [serialize_datetime(item) for item in obj]
    # å¤„ç†å¯¹è±¡ï¼ˆè½¬æ¢ __dict__ï¼‰
    elif hasattr(obj, '__dict__'):
        return serialize_datetime(obj.__dict__)
    # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
    else:
        return obj


# ============================================================================
# é«˜çº§æ£€ç´¢åŠŸèƒ½ï¼ˆBM25 + æ··åˆæ£€ç´¢ + Agentic æ£€ç´¢ï¼‰
# ============================================================================


def build_bm25_index(candidates: List[DocMemCell]):
    """æ„å»º BM25 ç´¢å¼•
    
    Args:
        candidates: å€™é€‰ MemCell åˆ—è¡¨
    
    Returns:
        (bm25_index, tokenized_docs, stemmer, stop_words)
    """
    try:
        import nltk
        from nltk.corpus import stopwords
        from nltk.stem import PorterStemmer
        from nltk.tokenize import word_tokenize
        from rank_bm25 import BM25Okapi
    except ImportError as e:
        print(f"[BM25] âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
        print("[BM25] æç¤º: è¯·å®‰è£… nltk å’Œ rank_bm25")
        print("  pip install nltk rank_bm25")
        return None, None, None, None
    
    # ç¡®ä¿ NLTK æ•°æ®å·²ä¸‹è½½
    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt", quiet=True)
    
    try:
        nltk.data.find("tokenizers/punkt_tab")
    except LookupError:
        nltk.download("punkt_tab", quiet=True)
    
    try:
        nltk.data.find("corpora/stopwords")
    except LookupError:
        nltk.download("stopwords", quiet=True)
    
    stemmer = PorterStemmer()
    stop_words = set(stopwords.words("english"))
    
    # æå–æ–‡æœ¬å¹¶åˆ†è¯
    tokenized_docs = []
    for mem in candidates:
        # ä¼˜å…ˆä½¿ç”¨ episodeï¼Œå›é€€åˆ° summary
        text = getattr(mem, "episode", None) or getattr(mem, "summary", "") or ""
        tokens = word_tokenize(text.lower())
        processed_tokens = [
            stemmer.stem(token)
            for token in tokens
            if token.isalpha() and len(token) >= 2 and token not in stop_words
        ]
        tokenized_docs.append(processed_tokens)
    
    # æ„å»º BM25 ç´¢å¼•
    bm25 = BM25Okapi(tokenized_docs)
    
    return bm25, tokenized_docs, stemmer, stop_words


async def search_with_bm25(
    query: str,
    bm25,
    candidates: List[DocMemCell],
    stemmer,
    stop_words,
    top_k: int = 50
) -> List[tuple]:
    """ä½¿ç”¨ BM25 ç´¢å¼•æ‰§è¡Œæ£€ç´¢
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        bm25: BM25 ç´¢å¼•
        candidates: å€™é€‰ MemCell åˆ—è¡¨
        stemmer: è¯å¹²æå–å™¨
        stop_words: åœç”¨è¯é›†åˆ
        top_k: è¿”å›ç»“æœæ•°é‡
    
    Returns:
        [(MemCell, score), ...] æ’åºåçš„ç»“æœåˆ—è¡¨
    """
    if bm25 is None:
        return []
    
    try:
        from nltk.tokenize import word_tokenize
    except ImportError:
        return []
    
    # åˆ†è¯æŸ¥è¯¢
    tokens = word_tokenize(query.lower())
    tokenized_query = [
        stemmer.stem(token)
        for token in tokens
        if token.isalpha() and len(token) >= 2 and token not in stop_words
    ]
    
    if not tokenized_query:
        return []
    
    # è®¡ç®— BM25 åˆ†æ•°
    scores = bm25.get_scores(tokenized_query)
    
    # æ’åºå¹¶è¿”å› Top-K
    results = sorted(
        zip(candidates, scores),
        key=lambda x: x[1],
        reverse=True
    )[:top_k]
    
    return results


def reciprocal_rank_fusion(
    results1: List[tuple],
    results2: List[tuple],
    k: int = 60
) -> List[tuple]:
    """RRF èåˆä¸¤ä¸ªæ£€ç´¢ç»“æœ
    
    ä½¿ç”¨ Reciprocal Rank Fusion (RRF) ç®—æ³•èåˆä¸¤ä¸ªæ£€ç´¢ç»“æœã€‚
    RRF å…¬å¼: score = sum(1 / (k + rank))
    
    Args:
        results1: ç¬¬ä¸€ä¸ªæ£€ç´¢ç»“æœ [(doc, score), ...]
        results2: ç¬¬äºŒä¸ªæ£€ç´¢ç»“æœ [(doc, score), ...]
        k: RRF å‚æ•°ï¼ˆé»˜è®¤ 60ï¼‰
    
    Returns:
        èåˆåçš„ç»“æœ [(doc, rrf_score), ...]
    """
    doc_rrf_scores = {}  # {doc_id: rrf_score}
    doc_map = {}         # {doc_id: doc}
    
    # å¤„ç†ç¬¬ä¸€ä¸ªç»“æœé›†
    for rank, (doc, score) in enumerate(results1, start=1):
        doc_id = id(doc)
        if doc_id not in doc_map:
            doc_map[doc_id] = doc
        doc_rrf_scores[doc_id] = doc_rrf_scores.get(doc_id, 0.0) + 1.0 / (k + rank)
    
    # å¤„ç†ç¬¬äºŒä¸ªç»“æœé›†
    for rank, (doc, score) in enumerate(results2, start=1):
        doc_id = id(doc)
        if doc_id not in doc_map:
            doc_map[doc_id] = doc
        doc_rrf_scores[doc_id] = doc_rrf_scores.get(doc_id, 0.0) + 1.0 / (k + rank)
    
    # æŒ‰ RRF åˆ†æ•°æ’åº
    sorted_docs = sorted(doc_rrf_scores.items(), key=lambda x: x[1], reverse=True)
    
    # è½¬æ¢å› (doc, score) æ ¼å¼
    fused_results = [(doc_map[doc_id], rrf_score) for doc_id, rrf_score in sorted_docs]
    
    return fused_results


async def lightweight_retrieval(
    query: str,
    candidates: List[DocMemCell],
    embedding_config: EmbeddingConfig,
    emb_top_n: int = 50,
    bm25_top_n: int = 50,
    final_top_n: int = 20
) -> tuple:
    """è½»é‡çº§æ£€ç´¢ï¼ˆEmbedding + BM25 + RRF èåˆï¼‰
    
    æµç¨‹ï¼š
    1. å¹¶è¡Œæ‰§è¡Œ Embedding å’Œ BM25 æ£€ç´¢
    2. å„å– Top-N å€™é€‰
    3. ä½¿ç”¨ RRF èåˆ
    4. è¿”å› Top-K ç»“æœ
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        candidates: å€™é€‰ MemCell åˆ—è¡¨
        embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
        emb_top_n: Embedding æ£€ç´¢æ•°é‡
        bm25_top_n: BM25 æ£€ç´¢æ•°é‡
        final_top_n: æœ€ç»ˆè¿”å›æ•°é‡
    
    Returns:
        (results, metadata)
    """
    import time
    start_time = time.time()
    
    metadata = {
        "retrieval_mode": "lightweight",
        "emb_count": 0,
        "bm25_count": 0,
        "final_count": 0,
        "total_latency_ms": 0.0,
    }
    
    if not candidates:
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    
    # æ„å»º BM25 ç´¢å¼•
    bm25, tokenized_docs, stemmer, stop_words = build_bm25_index(candidates)
    
    # å¹¶è¡Œæ‰§è¡Œ Embedding å’Œ BM25 æ£€ç´¢
    vectorize_service = get_vectorize_service()
    
    # Embedding æ£€ç´¢
    emb_results = []
    try:
        query_vec = await vectorize_service.get_embedding(query)
        query_norm = np.linalg.norm(query_vec)
        
        if query_norm > 0:
            scores = []
            for mem in candidates:
                try:
                    doc_vec = np.array(mem.extend.get("embedding", []))
                    if len(doc_vec) > 0:
                        doc_norm = np.linalg.norm(doc_vec)
                        if doc_norm > 0:
                            sim = np.dot(query_vec, doc_vec) / (query_norm * doc_norm)
                            scores.append((mem, float(sim)))
                except:
                    continue
            
            emb_results = sorted(scores, key=lambda x: x[1], reverse=True)[:emb_top_n]
    except Exception as e:
        print(f"[Embedding] âš ï¸ æ£€ç´¢å¤±è´¥: {e}")
    
    metadata["emb_count"] = len(emb_results)
    
    # BM25 æ£€ç´¢
    bm25_results = []
    if bm25 is not None:
        bm25_results = await search_with_bm25(
            query, bm25, candidates, stemmer, stop_words, top_k=bm25_top_n
        )
    
    metadata["bm25_count"] = len(bm25_results)
    
    # RRF èåˆ
    if not emb_results and not bm25_results:
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    elif not emb_results:
        final_results = bm25_results[:final_top_n]
    elif not bm25_results:
        final_results = emb_results[:final_top_n]
    else:
        fused_results = reciprocal_rank_fusion(emb_results, bm25_results, k=60)
        final_results = fused_results[:final_top_n]
    
    metadata["final_count"] = len(final_results)
    metadata["total_latency_ms"] = (time.time() - start_time) * 1000
    
    return final_results, metadata


async def agentic_retrieval(
    query: str,
    candidates: List[DocMemCell],
    embedding_config: EmbeddingConfig,
    llm_provider,
    config,
) -> tuple:
    """Agentic å¤šè½®æ£€ç´¢ï¼ˆLLM å¼•å¯¼ï¼‰
    
    æµç¨‹ï¼š
    1. Round 1: è½»é‡çº§æ£€ç´¢ Top 20
    2. LLM åˆ¤æ–­å……åˆ†æ€§ï¼ˆä½¿ç”¨å‰ 5 æ¡ï¼‰
    3. å¦‚æœä¸å……åˆ†ï¼šç”Ÿæˆæ”¹è¿›æŸ¥è¯¢ â†’ Round 2 â†’ åˆå¹¶
    4. è¿”å›æœ€ç»ˆç»“æœ
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        candidates: å€™é€‰ MemCell åˆ—è¡¨
        embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
        llm_provider: LLM æä¾›è€…
        config: é…ç½®å¯¹è±¡
    
    Returns:
        (results, metadata)
    """
    import time
    start_time = time.time()
    
    metadata = {
        "retrieval_mode": "agentic",
        "is_multi_round": False,
        "round1_count": 0,
        "round2_count": 0,
        "is_sufficient": None,
        "reasoning": None,
        "final_count": 0,
        "total_latency_ms": 0.0,
    }
    
    if not candidates:
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    
    # ========== Round 1: è½»é‡çº§æ£€ç´¢ ==========
    round1_results, round1_meta = await lightweight_retrieval(
        query=query,
        candidates=candidates,
        embedding_config=embedding_config,
        emb_top_n=50,
        bm25_top_n=50,
        final_top_n=20
    )
    
    metadata["round1_count"] = len(round1_results)
    
    if not round1_results:
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    
    # ========== LLM å……åˆ†æ€§æ£€æŸ¥ï¼ˆä½¿ç”¨å‰ 5 æ¡ï¼‰==========
    top5_for_check = round1_results[:5]
    
    try:
        is_sufficient, reasoning = await check_sufficiency_simple(
            query=query,
            results=top5_for_check,
            llm_provider=llm_provider
        )
        
        metadata["is_sufficient"] = is_sufficient
        metadata["reasoning"] = reasoning
        
        # å¦‚æœå……åˆ†ï¼Œç›´æ¥è¿”å› Round 1 ç»“æœ
        if is_sufficient:
            metadata["final_count"] = len(round1_results)
            metadata["total_latency_ms"] = (time.time() - start_time) * 1000
            return round1_results, metadata
    
    except Exception as e:
        print(f"[Agentic] âš ï¸ LLM å……åˆ†æ€§æ£€æŸ¥å¤±è´¥: {e}ï¼Œå›é€€åˆ° Round 1 ç»“æœ")
        metadata["final_count"] = len(round1_results)
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return round1_results, metadata
    
    # ========== Round 2: ç”Ÿæˆæ”¹è¿›æŸ¥è¯¢å¹¶é‡æ–°æ£€ç´¢ ==========
    metadata["is_multi_round"] = True
    
    try:
        refined_query = await generate_refined_query_simple(
            original_query=query,
            results=top5_for_check,
            llm_provider=llm_provider
        )
        
        metadata["refined_query"] = refined_query
        
        # ä½¿ç”¨æ”¹è¿›æŸ¥è¯¢è¿›è¡Œç¬¬äºŒè½®æ£€ç´¢
        round2_results, round2_meta = await lightweight_retrieval(
            query=refined_query,
            candidates=candidates,
            embedding_config=embedding_config,
            emb_top_n=50,
            bm25_top_n=50,
            final_top_n=20
        )
        
        metadata["round2_count"] = len(round2_results)
        
        # åˆå¹¶ä¸¤è½®ç»“æœï¼ˆå»é‡ï¼‰
        round1_ids = {id(doc) for doc, _ in round1_results}
        round2_unique = [(doc, score) for doc, score in round2_results if id(doc) not in round1_ids]
        
        # Round1 Top20 + Round2 å»é‡åçš„æ–‡æ¡£ï¼ˆæœ€å¤š 40 ä¸ªï¼‰
        combined_results = round1_results.copy()
        needed_from_round2 = 40 - len(combined_results)
        combined_results.extend(round2_unique[:needed_from_round2])
        
        # å–å‰ 20 ä¸ªä½œä¸ºæœ€ç»ˆç»“æœ
        final_results = combined_results[:20]
        
        metadata["final_count"] = len(final_results)
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        
        return final_results, metadata
    
    except Exception as e:
        print(f"[Agentic] âš ï¸ Round 2 å¤±è´¥: {e}ï¼Œå›é€€åˆ° Round 1 ç»“æœ")
        metadata["final_count"] = len(round1_results)
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return round1_results, metadata


async def check_sufficiency_simple(
    query: str,
    results: List[tuple],
    llm_provider
) -> tuple:
    """ç®€åŒ–ç‰ˆå……åˆ†æ€§æ£€æŸ¥
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        results: æ£€ç´¢ç»“æœ [(MemCell, score), ...]
        llm_provider: LLM æä¾›è€…
    
    Returns:
        (is_sufficient, reasoning)
    """
    # æ„å»ºæç¤ºè¯
    memories_text = []
    for i, (mem, score) in enumerate(results, start=1):
        episode = getattr(mem, "episode", None) or getattr(mem, "summary", "")
        memories_text.append(f"[{i}] {episode}")
    
    memories_str = "\n".join(memories_text)
    
    prompt = f"""Given a user query and retrieved memories, determine if the memories are sufficient to answer the query.

User Query: {query}

Retrieved Memories:
{memories_str}

Task: Analyze if these memories contain enough information to answer the query.
- If YES: Return "SUFFICIENT" and briefly explain why
- If NO: Return "INSUFFICIENT" and briefly explain what's missing

Output format (JSON):
{{
  "is_sufficient": true/false,
  "reasoning": "brief explanation"
}}"""
    
    try:
        # è°ƒç”¨ LLM
        response = await llm_provider.generate(prompt)
        
        # è§£æ JSON å“åº”
        import json
        import re
        
        # å°è¯•æå– JSON
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group(0))
            is_sufficient = result.get("is_sufficient", False)
            reasoning = result.get("reasoning", "")
            return is_sufficient, reasoning
        else:
            # å›é€€ï¼šç®€å•åˆ¤æ–­
            if "sufficient" in response.lower():
                return True, "Memories appear sufficient"
            else:
                return False, "Memories appear insufficient"
    
    except Exception as e:
        print(f"[LLM] âš ï¸ å……åˆ†æ€§æ£€æŸ¥å¼‚å¸¸: {e}")
        # é»˜è®¤è®¤ä¸ºå……åˆ†ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        return True, "Sufficiency check failed, assuming sufficient"


async def generate_refined_query_simple(
    original_query: str,
    results: List[tuple],
    llm_provider
) -> str:
    """ç®€åŒ–ç‰ˆæŸ¥è¯¢æ”¹è¿›
    
    Args:
        original_query: åŸå§‹æŸ¥è¯¢
        results: æ£€ç´¢ç»“æœ [(MemCell, score), ...]
        llm_provider: LLM æä¾›è€…
    
    Returns:
        æ”¹è¿›åçš„æŸ¥è¯¢
    """
    # æ„å»ºæç¤ºè¯
    memories_text = []
    for i, (mem, score) in enumerate(results, start=1):
        episode = getattr(mem, "episode", None) or getattr(mem, "summary", "")
        memories_text.append(f"[{i}] {episode}")
    
    memories_str = "\n".join(memories_text)
    
    prompt = f"""Given a user query and insufficient retrieved memories, generate an improved query to find more relevant information.

Original Query: {original_query}

Retrieved Memories (insufficient):
{memories_str}

Task: Generate a better query that might retrieve more relevant memories.
- Focus on missing aspects
- Use synonyms or related terms
- Be more specific or more general as needed

Output format (JSON):
{{
  "refined_query": "your improved query here"
}}"""
    
    try:
        # è°ƒç”¨ LLM
        response = await llm_provider.generate(prompt)
        
        # è§£æ JSON å“åº”
        import json
        import re
        
        # å°è¯•æå– JSON
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group(0))
            refined_query = result.get("refined_query", original_query)
            return refined_query
        else:
            # å›é€€ï¼šè¿”å›åŸå§‹æŸ¥è¯¢
            return original_query
    
    except Exception as e:
        print(f"[LLM] âš ï¸ æŸ¥è¯¢æ”¹è¿›å¼‚å¸¸: {e}")
        # é»˜è®¤è¿”å›åŸå§‹æŸ¥è¯¢
        return original_query
