"""è®°å¿†å¢å¼ºå¯¹è¯è„šæœ¬ - åŸºäºå†å²è®°å¿†çš„æ™ºèƒ½å¯¹è¯åŠ©æ‰‹

æœ¬è„šæœ¬å®ç°äº†ä¸€ä¸ªæ”¯æŒè®°å¿†æ£€ç´¢å’Œä¸ªäººç”»åƒçš„å¯¹è¯ç³»ç»Ÿã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è‡ªåŠ¨åˆ—å‡ºå¯ç”¨çš„ç¾¤ç»„å¯¹è¯
2. åŠ è½½ç¾¤ç»„å†…æ‰€æœ‰ç”¨æˆ·çš„ä¸ªäººç”»åƒ
3. åŸºäºç”¨æˆ·è¾“å…¥æ£€ç´¢ç›¸å…³è®°å¿†
4. ç»“åˆè®°å¿†å’Œç”»åƒç”Ÿæˆå›ç­”
5. ç»´æŠ¤å¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®ï¼‰
6. å¯¹è¯å†å²æŒä¹…åŒ–ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰

æ”¯æŒçš„å‘½ä»¤ï¼š
- exit: é€€å‡ºå¯¹è¯ï¼ˆä¿å­˜å†å²ï¼‰
- clear: æ¸…ç©ºå½“å‰å¯¹è¯å†å²
- reload: é‡æ–°åŠ è½½è®°å¿†å’Œç”»åƒ
- help: æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

ä½¿ç”¨æ–¹æ³•ï¼š
    python chat_with_memory.py
"""

import asyncio
import json
import logging
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass

# ç¡®ä¿ src åŒ…å¯è¢«å‘ç°
PROJECT_ROOT = Path(__file__).resolve().parents[1]
src_path = str(PROJECT_ROOT / "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨è·¯å¾„ä¸­
project_root = str(PROJECT_ROOT)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å°è¯•å¯¼å…¥ readline ä»¥æ”¯æŒè¡Œç¼–è¾‘åŠŸèƒ½
try:
    import readline

    READLINE_AVAILABLE = True
except ImportError:
    READLINE_AVAILABLE = False

from memory_config import (
    ChatModeConfig,
    LLMConfig,
    EmbeddingConfig,
    MongoDBConfig,
    ScenarioType,
)
from memory_utils import (
    ensure_mongo_beanie_ready,
    query_all_groups_from_mongodb,
    query_memcells_by_group_and_time,
    load_user_profiles_from_dir,
    get_user_name_from_profile,
    VectorSimilarityStrategy,
)
from i18n_texts import I18nTexts
from src.memory_layer.llm.llm_provider import LLMProvider
from src.common_utils.datetime_utils import get_now_with_timezone
from src.common_utils.cli_ui import CLIUI
from dotenv import load_dotenv

load_dotenv()


# ============================================================================
# ç»“æ„åŒ–å“åº”æ•°æ®ç±»
# ============================================================================


@dataclass
class StructuredResponse:
    """LLM ç»“æ„åŒ–å“åº”æ•°æ®ç±»"""

    answer: str  # æœ€ç»ˆå›ç­”ï¼ˆç”¨æˆ·çœ‹åˆ°çš„ï¼‰
    reasoning: str  # æ¨ç†è¿‡ç¨‹ï¼ˆåå°è®°å½•ï¼‰
    references: List[str]  # å¼•ç”¨çš„è®°å¿†ç¼–å·
    confidence: str  # ç½®ä¿¡åº¦ï¼šhigh/medium/low
    additional_notes: str  # è¡¥å……è¯´æ˜

    @classmethod
    def from_json(cls, json_data: Dict[str, Any]) -> "StructuredResponse":
        """ä» JSON æ•°æ®åˆ›å»ºç»“æ„åŒ–å“åº”å¯¹è±¡

        Args:
            json_data: LLM è¿”å›çš„ JSON æ•°æ®

        Returns:
            StructuredResponse å¯¹è±¡
        """
        return cls(
            answer=json_data.get("answer", ""),
            reasoning=json_data.get("reasoning", ""),
            references=json_data.get("references", []),
            confidence=json_data.get("confidence", "medium"),
            additional_notes=json_data.get("additional_notes", ""),
        )

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "answer": self.answer,
            "reasoning": self.reasoning,
            "references": self.references,
            "confidence": self.confidence,
            "additional_notes": self.additional_notes,
        }


# ============================================================================
# è¯­è¨€é€‰æ‹©å™¨
# ============================================================================


class LanguageSelector:
    """è¯­è¨€é€‰æ‹©å™¨ - ç”¨äºé€‰æ‹©ç•Œé¢è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰"""

    @staticmethod
    def select_language() -> str:
        """äº¤äº’å¼é€‰æ‹©è¯­è¨€

        Returns:
            è¯­è¨€ä»£ç ï¼š"zh" æˆ– "en"
        """
        # ä½¿ç”¨ä¸´æ—¶çš„åŒè¯­æ ‡é¢˜
        print()
        print("=" * 60)
        print("  ğŸŒ  è¯­è¨€é€‰æ‹© / Language Selection")
        print("=" * 60)
        print()
        print("  [1] ä¸­æ–‡ (Chinese)")
        print("  [2] English")
        print()

        while True:
            try:
                choice = input("è¯·é€‰æ‹©è¯­è¨€ / Please select language [1-2]: ").strip()

                if not choice:
                    continue

                index = int(choice)
                if index == 1:
                    return "zh"
                elif index == 2:
                    return "en"
                else:
                    print("âŒ è¯·è¾“å…¥ 1 æˆ– 2 / Please enter 1 or 2\n")

            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­— / Please enter a valid number\n")
            except KeyboardInterrupt:
                print("\n")
                return "zh"  # é»˜è®¤ä¸­æ–‡


# ============================================================================
# åœºæ™¯æ¨¡å¼é€‰æ‹©å™¨
# ============================================================================


class ScenarioSelector:
    """åœºæ™¯æ¨¡å¼é€‰æ‹©å™¨ - ç”¨äºé€‰æ‹©åŠ©æ‰‹æ¨¡å¼æˆ–ç¾¤èŠæ¨¡å¼"""

    @staticmethod
    def select_scenario(texts: I18nTexts) -> Optional[ScenarioType]:
        """äº¤äº’å¼é€‰æ‹©åœºæ™¯æ¨¡å¼

        Args:
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡

        Returns:
            ScenarioType.ASSISTANT æˆ– ScenarioType.GROUP_CHATï¼Œå–æ¶ˆè¿”å› None
        """
        ui = CLIUI()
        print()
        ui.section_heading(texts.get("scenario_selection_title"))
        print()

        # æ˜¾ç¤ºæ¨¡å¼åˆ—è¡¨
        print(f"  [1] {texts.get('scenario_assistant')}")
        print(f"      {texts.get('scenario_assistant_desc')}")
        print()
        print(f"  [2] {texts.get('scenario_group_chat')}")
        print(f"      {texts.get('scenario_group_chat_desc')}")
        print()

        while True:
            try:
                choice = input(f"{texts.get('scenario_prompt')}: ").strip()

                if not choice:
                    continue

                index = int(choice)
                if index == 1:
                    ui.success(
                        f"âœ“ {texts.get('scenario_selected')}: {texts.get('scenario_assistant')}"
                    )
                    return ScenarioType.ASSISTANT
                elif index == 2:
                    ui.success(
                        f"âœ“ {texts.get('scenario_selected')}: {texts.get('scenario_group_chat')}"
                    )
                    return ScenarioType.GROUP_CHAT
                else:
                    ui.error(f"âœ— {texts.get('invalid_input_number')}")

            except ValueError:
                ui.error(f"âœ— {texts.get('invalid_input_number')}")
            except KeyboardInterrupt:
                print("\n")
                return None


# ============================================================================
# ç¾¤ç»„é€‰æ‹©å™¨
# ============================================================================


class GroupSelector:
    """ç¾¤ç»„é€‰æ‹©å™¨ - ç”¨äºåˆ—å‡ºå’Œé€‰æ‹©å¯ç”¨çš„ç¾¤ç»„å¯¹è¯"""

    @staticmethod
    async def list_available_groups() -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç¾¤ç»„

        Returns:
            ç¾¤ç»„åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"index": 1, "group_id": "xxx", "name": "xxx", "memcell_count": 76}, ...]
        """
        groups = await query_all_groups_from_mongodb()

        # ä¸ºæ¯ä¸ªç¾¤ç»„æ·»åŠ ç´¢å¼•å’Œè‡ªå®šä¹‰åç§°
        for idx, group in enumerate(groups, start=1):
            group["index"] = idx
            # ç¾¤ç»„è‡ªå®šä¹‰åç§°æ˜ å°„ï¼ˆå¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹ç¾¤ç»„æ˜¾ç¤ºåç§°ï¼‰
            group_id = group["group_id"]
            if group_id == "AIäº§å“ç¾¤":
                group["name"] = "group_chat"  # ä¿®æ”¹ç¾¤ç»„1çš„æ˜¾ç¤ºåç§°
            else:
                group["name"] = group_id  # å…¶ä»–ç¾¤ç»„ä½¿ç”¨åŸåç§°

        return groups

    @staticmethod
    async def select_group(
        groups: List[Dict[str, Any]], texts: I18nTexts
    ) -> Optional[str]:
        """äº¤äº’å¼é€‰æ‹©ç¾¤ç»„

        Args:
            groups: ç¾¤ç»„åˆ—è¡¨
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡

        Returns:
            é€‰ä¸­çš„ group_idï¼Œå¦‚æœå–æ¶ˆè¿”å› None
        """
        if not groups:
            ChatUI.print_error(texts.get("groups_not_found"), texts)
            print(f"{texts.get('groups_extract_hint')}\n")
            return None

        # æ˜¾ç¤ºç¾¤ç»„åˆ—è¡¨
        ChatUI.print_group_list(groups, texts)

        # ç”¨æˆ·è¾“å…¥
        while True:
            try:
                choice = input(
                    f"\n{texts.get('groups_select_prompt')} [1-{len(groups)}]: "
                ).strip()

                if not choice:
                    continue

                index = int(choice)
                if 1 <= index <= len(groups):
                    selected_group = groups[index - 1]
                    return selected_group["group_id"]
                else:
                    ChatUI.print_error(
                        texts.get("groups_select_range_error", min=1, max=len(groups)),
                        texts,
                    )

            except ValueError:
                ChatUI.print_error(texts.get("invalid_input_number"), texts)
            except KeyboardInterrupt:
                print("\n")
                ChatUI.print_info(texts.get("groups_selection_cancelled"), texts)
                return None


# ============================================================================
# æ£€ç´¢æ¨¡å¼é€‰æ‹©å™¨
# ============================================================================


class RetrievalModeSelector:
    """æ£€ç´¢æ¨¡å¼é€‰æ‹©å™¨ - ç”¨äºé€‰æ‹©è½»é‡çº§æˆ– Agentic æ£€ç´¢æ¨¡å¼"""

    @staticmethod
    def select_retrieval_mode(texts: I18nTexts) -> Optional[str]:
        """äº¤äº’å¼é€‰æ‹©æ£€ç´¢æ¨¡å¼

        Args:
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡

        Returns:
            "lightweight" æˆ– "agentic"ï¼Œå–æ¶ˆè¿”å› None
        """
        ui = CLIUI()
        print()
        ui.section_heading(texts.get("retrieval_mode_selection_title"))
        print()

        # æ˜¾ç¤ºæ¨¡å¼åˆ—è¡¨
        print(f"  [1] {texts.get('retrieval_mode_lightweight')}")
        print(f"      {texts.get('retrieval_mode_lightweight_desc')}")
        print()
        print(f"  [2] {texts.get('retrieval_mode_agentic')}")
        print(f"      {texts.get('retrieval_mode_agentic_desc')}")
        print()

        # æ˜¾ç¤ºå»ºè®®æç¤º
        ui.note(texts.get("retrieval_mode_lightweight_note"), icon="ğŸ’¡")
        ui.note(texts.get("retrieval_mode_agentic_note"), icon="ğŸ’¡")
        print()

        while True:
            try:
                choice = input(f"{texts.get('retrieval_mode_prompt')}: ").strip()

                if not choice:
                    continue

                index = int(choice)
                if index == 1:
                    ui.success(
                        f"âœ“ {texts.get('retrieval_mode_selected')}: {texts.get('retrieval_mode_lightweight')}"
                    )
                    return "lightweight"
                elif index == 2:
                    ui.success(
                        f"âœ“ {texts.get('retrieval_mode_selected')}: {texts.get('retrieval_mode_agentic')}"
                    )
                    return "agentic"
                else:
                    ui.error(f"âœ— {texts.get('invalid_input_number')}")

            except ValueError:
                ui.error(f"âœ— {texts.get('invalid_input_number')}")
            except KeyboardInterrupt:
                print("\n")
                return None


# ============================================================================
# å¯¹è¯ä¼šè¯ç®¡ç†
# ============================================================================


class ChatSession:
    """å¯¹è¯ä¼šè¯ç®¡ç†å™¨ - ç®¡ç†å•ä¸ªç¾¤ç»„çš„å¯¹è¯ä¼šè¯"""

    def __init__(
        self,
        group_id: str,
        config: ChatModeConfig,
        llm_config: LLMConfig,
        embedding_config: EmbeddingConfig,
        scenario_type: ScenarioType,
        retrieval_mode: str,  # ğŸ”¥ æ–°å¢ï¼šæ£€ç´¢æ¨¡å¼ï¼ˆ"lightweight" æˆ– "agentic"ï¼‰
        texts: I18nTexts,
    ):
        """åˆå§‹åŒ–å¯¹è¯ä¼šè¯

        Args:
            group_id: ç¾¤ç»„ ID
            config: å¯¹è¯æ¨¡å¼é…ç½®
            llm_config: LLM é…ç½®
            embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
            scenario_type: åœºæ™¯ç±»å‹ï¼ˆåŠ©æ‰‹/ç¾¤èŠï¼‰
            retrieval_mode: æ£€ç´¢æ¨¡å¼ï¼ˆ"lightweight" æˆ– "agentic"ï¼‰
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        self.group_id = group_id
        self.config = config
        self.llm_config = llm_config
        self.embedding_config = embedding_config
        self.scenario_type = scenario_type  # è¿è¡Œæ—¶åœºæ™¯ç±»å‹
        self.retrieval_mode = retrieval_mode  # ğŸ”¥ è¿è¡Œæ—¶æ£€ç´¢æ¨¡å¼
        self.texts = texts  # å›½é™…åŒ–æ–‡æœ¬

        # ä¼šè¯çŠ¶æ€
        self.user_profiles: Dict[str, Dict] = {}  # æ‰€æœ‰ç”¨æˆ·çš„ Profile
        self.conversation_history: List[Tuple[str, str]] = (
            []
        )  # [(user_q, assistant_a), ...]
        self.memcell_count: int = 0

        # LLM å’Œæ£€ç´¢ç­–ç•¥
        self.llm_provider: Optional[LLMProvider] = None
        self.retrieval_strategy: Optional[VectorSimilarityStrategy] = None

        # æœ€åä¸€æ¬¡ç»“æ„åŒ–å“åº”ï¼ˆç”¨äºæŸ¥çœ‹æ¨ç†è¿‡ç¨‹ï¼‰
        self.last_structured_response: Optional[StructuredResponse] = None
        
        # ğŸ”¥ æœ€åä¸€æ¬¡æ£€ç´¢å…ƒæ•°æ®ï¼ˆç”¨äºæ˜¾ç¤ºæ£€ç´¢ä¿¡æ¯ï¼‰
        self.last_retrieval_metadata: Optional[Dict[str, Any]] = None

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–ä¼šè¯ï¼ˆåŠ è½½ Profileã€åˆ›å»º LLM Provider ç­‰ï¼‰

        Returns:
            åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            # 1. åŠ è½½ç”¨æˆ· Profile
            # è‡ªå®šä¹‰ç¾¤ç»„åç§°æ˜ å°„ï¼ˆä¸ GroupSelector ä¿æŒä¸€è‡´ï¼‰
            display_name = (
                "group_chat" if self.group_id == "AIäº§å“ç¾¤" else self.group_id
            )
            print(
                f"\n[{self.texts.get('loading_label')}] {self.texts.get('loading_group_data', name=display_name)}"
            )

            # ğŸ”¥ æ ¹æ®è¿è¡Œæ—¶çš„ scenario_type å’Œè¯­è¨€åŠ¨æ€ç¡®å®š Profile è·¯å¾„
            # è·¯å¾„æ ¼å¼ï¼šmemcell_outputs/{scenario}_{language}/profiles[_companion]/
            scenario_name = (
                "assistant" if self.scenario_type == ScenarioType.ASSISTANT else "group_chat"
            )
            scenario_dir = self.config.memcell_output_dir / f"{scenario_name}_{self.texts.language}"
            
            # Profile å­ç›®å½•ï¼šç¾¤èŠç”¨ profiles/ï¼ŒåŠ©æ‰‹ç”¨ profiles_companion/
            if self.scenario_type == ScenarioType.ASSISTANT:
                profiles_dir = scenario_dir / "profiles_companion"
            else:
                profiles_dir = scenario_dir / "profiles"
            
            self.user_profiles = load_user_profiles_from_dir(profiles_dir)

            if not self.user_profiles:
                print(
                    f"[{self.texts.get('warning_label')}] {self.texts.get('loading_profiles_warning')}"
                )
                print(
                    f"[{self.texts.get('hint_label')}] {self.texts.get('loading_profiles_hint')}"
                )
            else:
                user_names = [
                    get_user_name_from_profile(p) or uid
                    for uid, p in self.user_profiles.items()
                ]
                print(
                    f"[{self.texts.get('loading_label')}] {self.texts.get('loading_profiles_success', count=len(self.user_profiles), names=', '.join(user_names))} âœ…"
                )

            # 2. ç»Ÿè®¡ MemCell æ•°é‡ï¼ˆæŸ¥è¯¢æœ€è¿‘ä¸€å¹´çš„æ•°æ®ï¼‰
            now = get_now_with_timezone()
            start_date = now - timedelta(days=self.config.time_range_days)
            memcells = await query_memcells_by_group_and_time(
                self.group_id, start_date, now
            )
            self.memcell_count = len(memcells)
            print(
                f"[{self.texts.get('loading_label')}] {self.texts.get('loading_memories_success', count=self.memcell_count)} âœ…"
            )

            # 3. åŠ è½½å¯¹è¯å†å²
            loaded_history_count = await self.load_conversation_history()
            if loaded_history_count > 0:
                print(
                    f"[{self.texts.get('loading_label')}] {self.texts.get('loading_history_success', count=loaded_history_count)} âœ…"
                )
            else:
                print(
                    f"[{self.texts.get('loading_label')}] {self.texts.get('loading_history_new')} âœ…"
                )

            # 4. åˆ›å»º LLM Provider
            self.llm_provider = LLMProvider(
                self.llm_config.provider,
                model=self.llm_config.model,
                api_key=self.llm_config.api_key,
                base_url=self.llm_config.base_url,
                temperature=self.llm_config.temperature,
                max_tokens=self.llm_config.max_tokens,
            )

            # 5. åˆ›å»ºæ£€ç´¢ç­–ç•¥
            self.retrieval_strategy = VectorSimilarityStrategy(self.embedding_config)

            print(
                f"\n[{self.texts.get('hint_label')}] {self.texts.get('loading_help_hint')}\n"
            )
            return True

        except Exception as e:
            print(
                f"\n[{self.texts.get('error_label')}] {self.texts.get('session_init_error', error=str(e))}"
            )
            import traceback

            traceback.print_exc()
            return False

    async def load_conversation_history(self) -> int:
        """ä»æ–‡ä»¶åŠ è½½å¯¹è¯å†å²

        Returns:
            åŠ è½½çš„å¯¹è¯è½®æ•°
        """
        try:
            # è‡ªå®šä¹‰ç¾¤ç»„åç§°æ˜ å°„ï¼ˆä¸ GroupSelector ä¿æŒä¸€è‡´ï¼‰
            display_name = (
                "group_chat" if self.group_id == "AIäº§å“ç¾¤" else self.group_id
            )

            # æŸ¥æ‰¾æœ€æ–°çš„å†å²æ–‡ä»¶ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§æ–‡ä»¶åæ ¼å¼ï¼‰
            history_files = []
            # æŸ¥æ‰¾æ–°æ ¼å¼çš„æ–‡ä»¶å
            new_files = list(
                self.config.chat_history_dir.glob(f"{display_name}_*.json")
            )
            # æŸ¥æ‰¾æ—§æ ¼å¼çš„æ–‡ä»¶åï¼ˆä¸ºäº†å‘åå…¼å®¹ï¼‰
            old_files = list(
                self.config.chat_history_dir.glob(f"{self.group_id}_*.json")
            )
            # æŸ¥æ‰¾æ›´æ—§çš„æ ¼å¼ï¼ˆå…¼å®¹ä¹‹å‰çš„æ—§åç§°ï¼‰
            very_old_files = list(
                self.config.chat_history_dir.glob("EverMem 928äº¤ä»˜ç¾¤_*.json")
            )

            history_files = sorted(
                new_files + old_files + very_old_files, reverse=True
            )  # æœ€æ–°çš„åœ¨å‰

            if not history_files:
                return 0

            latest_file = history_files[0]

            with latest_file.open("r", encoding="utf-8") as fp:
                data = json.load(fp)

            # åŠ è½½æœ€è¿‘ N è½®å¯¹è¯
            history = data.get("conversation_history", [])
            self.conversation_history = [
                (item["user_input"], item["assistant_response"])
                for item in history[-self.config.conversation_history_size :]
            ]

            return len(self.conversation_history)

        except Exception as e:
            print(
                f"[{self.texts.get('warning_label')}] {self.texts.get('loading_history_new')}: {e}"
            )
            return 0

    async def save_conversation_history(self) -> None:
        """ä¿å­˜å¯¹è¯å†å²åˆ°æ–‡ä»¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰"""
        try:
            # è‡ªå®šä¹‰ç¾¤ç»„åç§°æ˜ å°„ï¼ˆä¸ GroupSelector ä¿æŒä¸€è‡´ï¼‰
            display_name = (
                "group_chat" if self.group_id == "AIäº§å“ç¾¤" else self.group_id
            )
            # æ–‡ä»¶åï¼šgroup_chat_001_2025-10-24_16-30.json
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
            filename = f"{display_name}_{timestamp}.json"
            filepath = self.config.chat_history_dir / filename

            # æ„å»ºæ•°æ®
            data = {
                "group_id": self.group_id,
                "last_updated": datetime.now().isoformat(),
                "conversation_history": [
                    {
                        "timestamp": datetime.now().isoformat(),
                        "user_input": user_q,
                        "assistant_response": assistant_a,
                    }
                    for user_q, assistant_a in self.conversation_history
                ],
            }

            # ä¿å­˜æ–‡ä»¶
            with filepath.open("w", encoding="utf-8") as fp:
                json.dump(data, fp, ensure_ascii=False, indent=2)

            print(f"[{self.texts.get('save_label')}] {filename} âœ…")

        except Exception as e:
            print(f"[{self.texts.get('error_label')}] {e}")

    async def retrieve_memories(self, query: str) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆæ”¯æŒå¤šç§æ£€ç´¢æ¨¡å¼ï¼‰

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
        """
        # æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„ MemCell
        now = get_now_with_timezone()
        start_date = now - timedelta(days=self.config.time_range_days)

        candidates = await query_memcells_by_group_and_time(
            self.group_id, start_date, now
        )

        if not candidates:
            self.last_retrieval_metadata = {"retrieval_mode": self.retrieval_mode, "total_latency_ms": 0.0}
            return []
        
        # ğŸ”¥ æ ¹æ®æ£€ç´¢æ¨¡å¼æ‰§è¡Œä¸åŒçš„æ£€ç´¢é€»è¾‘
        if self.retrieval_mode == "lightweight":
            # è½»é‡çº§æ£€ç´¢ï¼šEmbedding + BM25 + RRF èåˆ
            from demo.memory_utils import lightweight_retrieval
            
            results_tuples, metadata = await lightweight_retrieval(
                query=query,
                candidates=candidates,
                embedding_config=self.embedding_config,
                emb_top_n=self.config.lightweight_emb_top_n,
                bm25_top_n=self.config.lightweight_bm25_top_n,
                final_top_n=self.config.lightweight_final_top_n
            )
            
            # ä¿å­˜å…ƒæ•°æ®
            self.last_retrieval_metadata = metadata
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            results = []
            for mem, score in results_tuples:
                item = {
                    "event_id": str(getattr(mem, "event_id", getattr(mem, "id", ""))),
                    "timestamp": (
                        getattr(mem, "timestamp", None).isoformat()
                        if getattr(mem, "timestamp", None)
                        else None
                    ),
                    "group_id": getattr(mem, "group_id", None),
                    "subject": getattr(mem, "subject", None),
                    "summary": getattr(mem, "summary", None),
                    "episode": getattr(mem, "episode", None),
                    "participants": getattr(mem, "participants", []),
                    "score": round(score, 4),
                }
                results.append(item)
            
            return results
        
        elif self.retrieval_mode == "agentic":
            # Agentic æ£€ç´¢ï¼šLLM å¼•å¯¼çš„å¤šè½®æ£€ç´¢
            from demo.memory_utils import agentic_retrieval
            
            results_tuples, metadata = await agentic_retrieval(
                query=query,
                candidates=candidates,
                embedding_config=self.embedding_config,
                llm_provider=self.llm_provider,
                config=self.config
            )
            
            # ä¿å­˜å…ƒæ•°æ®
            self.last_retrieval_metadata = metadata
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            results = []
            for mem, score in results_tuples:
                item = {
                    "event_id": str(getattr(mem, "event_id", getattr(mem, "id", ""))),
                    "timestamp": (
                        getattr(mem, "timestamp", None).isoformat()
                        if getattr(mem, "timestamp", None)
                        else None
                    ),
                    "group_id": getattr(mem, "group_id", None),
                    "subject": getattr(mem, "subject", None),
                    "summary": getattr(mem, "summary", None),
                    "episode": getattr(mem, "episode", None),
                    "participants": getattr(mem, "participants", []),
                    "score": round(score, 4),
                }
                results.append(item)
            
            return results
        
        else:
            # å›é€€åˆ°é»˜è®¤æ£€ç´¢ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            results = await self.retrieval_strategy.retrieve(
                query=query, candidates=candidates, top_k=self.config.top_k_memories
            )

            if self.scenario_type == ScenarioType.ASSISTANT:
                results_semantic = await self.retrieval_strategy.retrieve_semantic(
                    query=query,
                    candidates=candidates,
                    date_query=datetime.strptime("2024-10-27", "%Y-%m-%d"),
                    top_k=self.config.top_k_memories,
                )
                results = results + results_semantic
            
            self.last_retrieval_metadata = {"retrieval_mode": "default", "total_latency_ms": 0.0}
            return results

    def build_prompt(
        self, user_query: str, memories: List[Dict[str, Any]], profiles: Dict[str, Dict]
    ) -> List[Dict[str, str]]:
        """æ„å»º Chat Messages æ ¼å¼çš„ Prompt

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            memories: æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
            profiles: æ‰€æœ‰ç”¨æˆ·çš„ Profile

        Returns:
            Chat Messages åˆ—è¡¨
        """
        messages = []

        # 1. System Message - è§’è‰²å®šä¹‰å’Œ JSON è¾“å‡ºè¦æ±‚ï¼ˆæ ¹æ®è¯­è¨€é€‰æ‹©ï¼‰
        lang_key = "zh" if self.texts.language == "zh" else "en"
        system_content = self.texts.get(f"prompt_system_role_{lang_key}")

        messages.append({"role": "system", "content": system_content})

        # 2. Profile Context - ä¸ªäººç”»åƒï¼ˆæ‰€æœ‰ç”¨æˆ·ï¼‰

        if profiles:
            if self.scenario_type == ScenarioType.ASSISTANT:
                profile_data = profiles.get("user_001")
                if not profile_data:
                    # å›é€€ï¼šä¼˜å…ˆé€‰æ‹©é assistant çš„ç”¨æˆ·ç”»åƒ
                    for _uid, _pf in profiles.items():
                        if (_pf or {}).get("user_name") != "assistant":
                            profile_data = _pf
                            break
                if profile_data:
                    profile_content = self._sanitize_companion_profile_for_prompt(
                        profile_data
                    )
                    messages.append(
                        {
                            "role": "system",
                            "content": self.texts.get(
                                f"prompt_profile_prefix_{lang_key}"
                            )
                            + profile_content,
                        }
                    )

            elif self.scenario_type == ScenarioType.GROUP_CHAT:
                # print(profiles)
                profile_list = []
                for user_id, profile in profiles.items():
                    user_name = get_user_name_from_profile(profile) or user_id
                    # print(user_name)
                    user_profile = profiles[user_id]
                    profile_list.append(
                        json.dumps(user_profile, ensure_ascii=False, indent=2)
                    )

                if profile_list != []:
                    profile_content = self.texts.get(
                        f"prompt_profile_prefix_{lang_key}"
                    ) + "\n".join(profile_list)
                    # print(profile_content)
                    messages.append({"role": "system", "content": profile_content})

        # 3. Retrieved Memories - æ£€ç´¢åˆ°çš„è®°å¿†
        if memories:
            memory_lines = []
            for i, mem in enumerate(memories, start=1):
                timestamp = mem.get("timestamp", "")[:10]  # åªå–æ—¥æœŸéƒ¨åˆ†
                subject = mem.get("subject", "")
                summary = mem.get("summary", "")
                episode = mem.get("episode", "")
                # è¯¦ç»†ç‰ˆï¼šæä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥ä¾¿æ¨ç†
                # æ ¼å¼ï¼š[åºå·] æ—¥æœŸ | ä¸»é¢˜ï¼šxxx | å†…å®¹ï¼šxxx
                parts = [
                    f"[{i}] {self.texts.get('prompt_memory_date', date=timestamp)}"
                ]
                if subject:
                    parts.append(
                        self.texts.get("prompt_memory_subject", subject=subject)
                    )
                if summary:
                    parts.append(
                        self.texts.get("prompt_memory_content", content=summary)
                    )
                if episode:
                    parts.append(
                        self.texts.get("prompt_memory_episode", episode=episode)
                    )
                memory_lines.append(" | ".join(parts))

            memory_content = self.texts.get("prompt_memories_prefix") + "\n".join(
                memory_lines
            )
            messages.append({"role": "system", "content": memory_content})

        # 4. Conversation History - å¯¹è¯å†å²ï¼ˆæœ€è¿‘ N è½®ï¼‰
        for user_q, assistant_a in self.conversation_history[
            -self.config.conversation_history_size :
        ]:
            messages.append({"role": "user", "content": user_q})
            messages.append({"role": "assistant", "content": assistant_a})

        # 5. Current Question - å½“å‰é—®é¢˜
        messages.append({"role": "user", "content": user_query})

        return messages

    def _sanitize_companion_profile_for_prompt(self, profile: Dict[str, Any]) -> str:
        """ç”Ÿæˆç”¨äº Prompt çš„ç²¾ç®€ JSONï¼ˆå»æ‰ nullã€å»æ‰ reasoningã€å»æ‰æ‰€æœ‰ evidence* ç›¸å…³å­—æ®µï¼‰ã€‚"""

        def is_evidence_key(key: str) -> bool:
            try:
                return "evidence" in key.lower()
            except Exception:
                return False

        def cleanse(obj: Any) -> Any:
            if isinstance(obj, dict):
                cleaned: Dict[str, Any] = {}
                for k, v in obj.items():
                    # ç§»é™¤æ¨ç†å­—æ®µä¸ç©ºå€¼
                    if k in ("output_reasoning", "reasoning", "outputReasoning"):
                        continue
                    if v is None:
                        continue
                    cv = cleanse(v)
                    if cv is not None:
                        cleaned[k] = cv
                return cleaned
            if isinstance(obj, list):
                new_list: List[Any] = []
                for item in obj:
                    if item is None:
                        continue
                    if isinstance(item, dict):
                        filtered = {
                            kk: vv
                            for kk, vv in item.items()
                            if (not is_evidence_key(kk)) and vv is not None
                        }
                        cleaned_item = cleanse(filtered)
                        if cleaned_item:
                            new_list.append(cleaned_item)
                    else:
                        new_list.append(item)
                return new_list
            return obj

        sanitized = cleanse(profile)
        try:
            return json.dumps(sanitized, ensure_ascii=False, indent=2)
        except Exception:
            return json.dumps(profile, ensure_ascii=False)

    async def chat(self, user_input: str) -> str:
        """æ ¸å¿ƒå¯¹è¯é€»è¾‘

        Args:
            user_input: ç”¨æˆ·è¾“å…¥

        Returns:
            åŠ©æ‰‹å›ç­”
        """
        # 1. æ£€ç´¢è®°å¿†ï¼ˆæ–°æ–¹æ³•è¿”å›å•ä¸ªåˆ—è¡¨ï¼‰
        memories = await self.retrieve_memories(user_input)

        # 2. æ˜¾ç¤ºæ£€ç´¢ç»“æœï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰- åªæ˜¾ç¤ºå‰ 5 æ¡
        if self.config.show_retrieved_memories:
            if memories:
                # ğŸ”¥ æ˜¾ç¤ºæ£€ç´¢æ¨¡å¼å’Œè€—æ—¶ä¿¡æ¯
                retrieval_mode_display = self.retrieval_mode
                ChatUI.print_retrieved_memories(
                    memories[:5],
                    total_count=len(memories),
                    texts=self.texts,
                    retrieval_metadata=self.last_retrieval_metadata,  # ğŸ”¥ ä¼ é€’å…ƒæ•°æ®
                )
        
        # 3. æ„å»º Promptï¼ˆä½¿ç”¨å…¨éƒ¨è®°å¿†ï¼‰
        messages = self.build_prompt(user_input, memories, self.user_profiles)

        # 4. æ˜¾ç¤ºç”Ÿæˆè¿›åº¦æç¤º
        ChatUI.print_generating_indicator(self.texts)

        # 5. è°ƒç”¨ LLM
        try:
            # å°† messages æ ¼å¼è½¬æ¢ä¸ºå•ä¸ª promptï¼ˆLLMProvider.generate éœ€è¦å­—ç¬¦ä¸²ï¼‰
            # æˆ–è€…ç›´æ¥ä½¿ç”¨åº•å±‚ provider çš„ chat åŠŸèƒ½
            if hasattr(self.llm_provider, 'provider') and hasattr(
                self.llm_provider.provider, 'chat_with_messages'
            ):
                # å¦‚æœ provider æ”¯æŒç›´æ¥ä½¿ç”¨ messages
                raw_response = await self.llm_provider.provider.chat_with_messages(
                    messages
                )
            else:
                # å›é€€æ–¹æ¡ˆï¼šå°† messages è½¬æ¢ä¸ºå•ä¸ª prompt
                prompt_parts = []
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    if role == "system":
                        prompt_parts.append(f"System: {content}")
                    elif role == "user":
                        prompt_parts.append(f"User: {content}")
                    elif role == "assistant":
                        prompt_parts.append(f"Assistant: {content}")

                prompt = "\n\n".join(prompt_parts)
                raw_response = await self.llm_provider.generate(prompt)

            raw_response = raw_response.strip()

            # 6. æ¸…é™¤ç”Ÿæˆè¿›åº¦æç¤ºï¼Œæ˜¾ç¤ºå®Œæˆæ ‡è¯†
            ChatUI.print_generation_complete(self.texts)

            # 7. è§£æ JSON å“åº”
            structured_response = self.parse_llm_response(raw_response)

            if structured_response is None:
                # JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
                print(
                    f"[{self.texts.get('warning_label')}] LLM è¾“å‡ºé JSON æ ¼å¼ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ"
                )
                structured_response = self.create_fallback_response(raw_response)

            # 8. ä¿å­˜ç»“æ„åŒ–å“åº”
            self.last_structured_response = structured_response

            # 9. æ˜¾ç¤ºç»“æ„åŒ–ä¿¡æ¯ï¼ˆå…ƒæ•°æ®ï¼‰
            if self.config.show_reasoning_metadata:
                ChatUI.print_response_metadata(structured_response, self.texts)

            # 10. æå–ç”¨æˆ·çœ‹åˆ°çš„å›ç­”
            assistant_response = structured_response.answer

        except Exception as e:
            # æ¸…é™¤è¿›åº¦æç¤º
            ChatUI.clear_progress_indicator()
            error_msg = f"[{self.texts.get('error_label')}] {self.texts.get('chat_llm_error', error=str(e))}"
            print(f"\n{error_msg}")
            import traceback

            traceback.print_exc()
            return error_msg

        # 11. æ›´æ–°å¯¹è¯å†å²ï¼ˆå­˜å‚¨å®Œæ•´çš„ answerï¼‰
        self.conversation_history.append((user_input, assistant_response))

        # 12. ä¿æŒå†å²çª—å£å¤§å°
        if len(self.conversation_history) > self.config.conversation_history_size:
            self.conversation_history = self.conversation_history[
                -self.config.conversation_history_size :
            ]

        return assistant_response

    def parse_llm_response(self, raw_response: str) -> Optional[StructuredResponse]:
        """è§£æ LLM çš„ JSON å“åº”

        Args:
            raw_response: LLM åŸå§‹å“åº”æ–‡æœ¬

        Returns:
            StructuredResponse å¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å› None
        """
        try:
            # å°è¯•ç›´æ¥è§£æ JSON
            json_data = json.loads(raw_response.strip())
            return StructuredResponse.from_json(json_data)
        except json.JSONDecodeError:
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå– JSON éƒ¨åˆ†
            try:
                # æŸ¥æ‰¾ JSON ä»£ç å—ï¼ˆ```json ... ```ï¼‰
                import re

                json_match = re.search(
                    r'```json\s*(\{.*?\})\s*```', raw_response, re.DOTALL
                )
                if json_match:
                    json_str = json_match.group(1)
                    json_data = json.loads(json_str)
                    return StructuredResponse.from_json(json_data)

                # æŸ¥æ‰¾æ™®é€šä»£ç å—ï¼ˆ``` ... ```ï¼‰
                code_match = re.search(
                    r'```\s*(\{.*?\})\s*```', raw_response, re.DOTALL
                )
                if code_match:
                    json_str = code_match.group(1)
                    json_data = json.loads(json_str)
                    return StructuredResponse.from_json(json_data)

                # å°è¯•æŸ¥æ‰¾ { ... } ç»“æ„
                brace_match = re.search(r'\{.*\}', raw_response, re.DOTALL)
                if brace_match:
                    json_str = brace_match.group(0)
                    json_data = json.loads(json_str)
                    return StructuredResponse.from_json(json_data)

                return None
            except Exception as e:
                print(f"[è­¦å‘Š] JSON æå–å¤±è´¥: {e}")
                return None
        except Exception as e:
            print(f"[è­¦å‘Š] å“åº”è§£æå¼‚å¸¸: {e}")
            return None

    def create_fallback_response(self, raw_text: str) -> StructuredResponse:
        """åˆ›å»ºå›é€€å“åº”ï¼ˆå½“ JSON è§£æå¤±è´¥æ—¶ï¼‰

        Args:
            raw_text: åŸå§‹å“åº”æ–‡æœ¬

        Returns:
            StructuredResponse å¯¹è±¡
        """
        return StructuredResponse(
            answer=raw_text,
            reasoning="LLM æœªæŒ‰ JSON æ ¼å¼è¾“å‡ºï¼Œæ— æ³•æå–æ¨ç†è¿‡ç¨‹",
            references=[],
            confidence="low",
            additional_notes="æ³¨æ„ï¼šæ­¤å›ç­”æœªç»ç»“æ„åŒ–å¤„ç†",
        )

    def show_reasoning(self) -> None:
        """æ˜¾ç¤ºæœ€åä¸€æ¬¡å›ç­”çš„å®Œæ•´æ¨ç†è¿‡ç¨‹"""
        if self.last_structured_response is None:
            ChatUI.print_info(self.texts.get("cmd_reasoning_no_data"), self.texts)
            return

        ChatUI.print_full_reasoning(self.last_structured_response, self.texts)

    def clear_history(self) -> None:
        """æ¸…ç©ºå¯¹è¯å†å²"""
        count = len(self.conversation_history)
        self.conversation_history = []
        ChatUI.print_info(self.texts.get("cmd_clear_done", count=count), self.texts)

    async def reload_data(self) -> None:
        """é‡æ–°åŠ è½½è®°å¿†å’Œç”»åƒ"""
        # è‡ªå®šä¹‰ç¾¤ç»„åç§°æ˜ å°„ï¼ˆä¸ GroupSelector ä¿æŒä¸€è‡´ï¼‰
        display_name = "group_chat" if self.group_id == "AIäº§å“ç¾¤" else self.group_id

        ui = ChatUI._ui()
        print()
        ui.note(self.texts.get("cmd_reload_refreshing", name=display_name), icon="ğŸ”„")

        # ğŸ”¥ æ ¹æ®è¿è¡Œæ—¶çš„ scenario_type å’Œè¯­è¨€åŠ¨æ€ç¡®å®š Profile è·¯å¾„
        scenario_name = (
            "assistant" if self.scenario_type == ScenarioType.ASSISTANT else "group_chat"
        )
        scenario_dir = self.config.memcell_output_dir / f"{scenario_name}_{self.texts.language}"
        
        if self.scenario_type == ScenarioType.ASSISTANT:
            profiles_dir = scenario_dir / "profiles_companion"
        else:
            profiles_dir = scenario_dir / "profiles"
        
        self.user_profiles = load_user_profiles_from_dir(profiles_dir)

        # é‡æ–°ç»Ÿè®¡ MemCell æ•°é‡
        now = get_now_with_timezone()
        start_date = now - timedelta(days=self.config.time_range_days)
        memcells = await query_memcells_by_group_and_time(
            self.group_id, start_date, now
        )
        self.memcell_count = len(memcells)

        print()
        ui.success(
            f"âœ“ {self.texts.get('cmd_reload_complete', users=len(self.user_profiles), memories=self.memcell_count)}"
        )
        print()


# ============================================================================
# ç¾è§‚çš„ç»ˆç«¯ç•Œé¢
# ============================================================================


class ChatUI:
    """ç»ˆç«¯ç•Œé¢å·¥å…·ç±» - æä¾›ç¾è§‚çš„è¾“å‡ºæ ¼å¼"""

    @staticmethod
    def _ui() -> CLIUI:
        """è·å–ä¸€ä¸ªå®½åº¦è‡ªé€‚åº”ã€å¯é€‰å½©è‰²çš„ UI å®ä¾‹ã€‚

        é¢œè‰²å¼€å…³éµå¾ªç¯å¢ƒå˜é‡ï¼š
        - NO_COLOR: ä»»æ„å€¼è¡¨ç¤ºç¦ç”¨é¢œè‰²
        - CLI_UI_COLOR=0 ç¦ç”¨é¢œè‰²ï¼›å…¶ä»–å€¼æˆ–æœªè®¾ç½®åˆ™æ ¹æ® TTY è‡ªåŠ¨å¯ç”¨
        """
        return CLIUI()

    @staticmethod
    def clear_screen():
        """æ¸…ç©ºå±å¹•å†…å®¹

        ä½¿ç”¨ ANSI è½¬ä¹‰åºåˆ—æ¸…å±å¹¶å°†å…‰æ ‡ç§»åŠ¨åˆ°å·¦ä¸Šè§’ï¼Œ
        å…¼å®¹ Unix/Linux/macOS å’Œ Windows ç»ˆç«¯ã€‚
        """
        import os

        # ä¼˜å…ˆä½¿ç”¨ ANSI è½¬ä¹‰åºåˆ—ï¼ˆæ›´é€šç”¨ï¼‰
        print("\033[2J\033[H", end="")
        # åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
        import sys

        sys.stdout.flush()

    @staticmethod
    def print_banner(texts: I18nTexts):
        """æ‰“å°æ¬¢è¿æ¨ªå¹…

        Args:
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        ui = ChatUI._ui()
        print()
        ui.banner(texts.get("banner_title"), subtitle=texts.get("banner_subtitle"))
        if READLINE_AVAILABLE:
            ui.note(texts.get("readline_available"), icon="ğŸ’¡")
        else:
            ui.note(texts.get("readline_unavailable"), icon="ğŸ’¡")
        print()

    @staticmethod
    def print_group_list(groups: List[Dict[str, Any]], texts: I18nTexts):
        """æ˜¾ç¤ºç¾¤ç»„åˆ—è¡¨ï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰

        Args:
            groups: ç¾¤ç»„åˆ—è¡¨
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        ui = ChatUI._ui()
        print()
        ui.section_heading(texts.get("groups_available_title"))

        rows = []
        for group in groups:
            index = group["index"]
            group_id = group["group_id"]
            name = group.get("name", group_id)
            count = group["memcell_count"]
            count_text = f"ğŸ’¾ {count} " + (
                "memories" if texts.language == "en" else "æ¡è®°å¿†"
            )
            rows.append([f"[{index}]", group_id, f"ğŸ“ \"{name}\"", count_text])

        headers = [
            texts.get("table_header_index"),
            texts.get("table_header_group"),
            texts.get("table_header_name"),
            texts.get("table_header_count"),
        ]
        ui.table(headers=headers, rows=rows, aligns=["right", "left", "left", "right"])

    @staticmethod
    def print_retrieved_memories(
        memories: List[Dict[str, Any]],
        total_count: Optional[int],
        texts: I18nTexts,
        retrieval_metadata: Optional[Dict[str, Any]] = None,
    ):
        """æ˜¾ç¤ºæ£€ç´¢åˆ°çš„è®°å¿†ï¼ˆç®€æ´ç‰ˆï¼‰

        Args:
            memories: è®°å¿†åˆ—è¡¨ï¼ˆæ˜¾ç¤ºç”¨ï¼‰
            total_count: å®é™…æ£€ç´¢åˆ°çš„æ€»æ•°ï¼ˆä¿ç•™å‚æ•°ä»¥å…¼å®¹æ—§ä»£ç ï¼Œä½†ä¸å†ä½¿ç”¨ï¼‰
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
            retrieval_metadata: æ£€ç´¢å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        ui = ChatUI._ui()

        # ğŸ”¥ ç®€åŒ–æ ‡é¢˜ï¼šæ˜¾ç¤ºæ£€ç´¢å®Œæˆå’Œæ˜¾ç¤ºæ•°é‡
        heading = f"ğŸ” {texts.get('retrieval_complete')}"
        shown_count = len(memories)
        if shown_count > 0:
            heading += f" - {texts.get('retrieval_showing', shown=shown_count)}"
        
        # ğŸ”¥ å¦‚æœæœ‰å…ƒæ•°æ®ï¼Œæ˜¾ç¤ºæ£€ç´¢æ¨¡å¼å’Œè€—æ—¶
        if retrieval_metadata:
            retrieval_mode = retrieval_metadata.get("retrieval_mode", "default")
            latency_ms = retrieval_metadata.get("total_latency_ms", 0.0)
            is_multi_round = retrieval_metadata.get("is_multi_round", False)
            
            if retrieval_mode == "lightweight":
                mode_text = texts.get("retrieval_mode_lightweight")
            elif retrieval_mode == "agentic":
                mode_text = texts.get("retrieval_mode_agentic")
                if is_multi_round:
                    mode_text += f" ({texts.get('retrieval_multi_round')})"
                else:
                    mode_text += f" ({texts.get('retrieval_single_round')})"
            else:
                mode_text = "Default"
            
            heading += f" | {mode_text} | {texts.get('retrieval_latency', latency=int(latency_ms))}"

        ui.section_heading(heading)

        lines = []
        for i, mem in enumerate(memories, start=1):
            timestamp = mem.get("timestamp", "")[:10]
            subject = mem.get("subject", "")
            summary = mem.get("summary", "")
            content = subject or summary or ""
            lines.append(f"ğŸ“Œ [{i:2d}]  {timestamp}  â”‚  {content}")
        if lines:
            ui.panel(lines)

    @staticmethod
    def print_generating_indicator(texts: I18nTexts):
        """æ˜¾ç¤ºç”Ÿæˆè¿›åº¦æç¤º

        Args:
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        ui = ChatUI._ui()
        print()
        ui.note(f"ğŸ¤” {texts.get('chat_generating')}", icon="â³")

    @staticmethod
    def print_generation_complete(texts: I18nTexts):
        """æ¸…é™¤ç”Ÿæˆæç¤ºå¹¶æ˜¾ç¤ºå®Œæˆæ ‡è¯†

        Args:
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        # ä½¿ç”¨ ANSI è½¬ä¹‰åºåˆ—æ¸…é™¤ä¸Šä¸€è¡Œå¹¶ç§»åŠ¨å…‰æ ‡
        print("\r\033[K", end="")  # æ¸…é™¤å½“å‰è¡Œ
        print("\033[A\033[K", end="")  # å‘ä¸Šç§»åŠ¨ä¸€è¡Œå¹¶æ¸…é™¤
        print("\033[A\033[K", end="")  # å†å‘ä¸Šç§»åŠ¨ä¸€è¡Œå¹¶æ¸…é™¤ï¼ˆå¤„ç†ä¸¤è¡Œçš„æç¤ºï¼‰
        ui = ChatUI._ui()
        ui.success(f"âœ“ {texts.get('chat_generation_complete')}")

    @staticmethod
    def clear_progress_indicator():
        """æ¸…é™¤è¿›åº¦æç¤ºï¼ˆå‡ºé”™æ—¶ä½¿ç”¨ï¼‰"""
        # æ¸…é™¤è¿›åº¦æç¤ºè¡Œ
        print("\r\033[K", end="")
        print("\033[A\033[K", end="")
        print("\033[A\033[K", end="")

    @staticmethod
    def print_response_metadata(response: "StructuredResponse", texts: I18nTexts):
        """æ˜¾ç¤ºç»“æ„åŒ–å“åº”çš„å…ƒæ•°æ®

        Args:
            response: ç»“æ„åŒ–å“åº”å¯¹è±¡
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        ui = ChatUI._ui()
        confidence_emoji = {"high": "ğŸŸ¢", "medium": "ğŸŸ¡", "low": "ğŸ”´"}
        emoji = confidence_emoji.get(response.confidence, "âšª")
        refs = (
            f"ğŸ“ {texts.get('response_references')}: {', '.join(response.references)}"
            if response.references
            else ""
        )
        line = (
            f"{emoji} {texts.get('response_confidence')}: {response.confidence.upper()}"
        )
        if refs:
            line += f"  |  {refs}"
        print()
        ui.text(line)

    @staticmethod
    def print_assistant_response(response: str, texts: I18nTexts):
        """æ˜¾ç¤ºåŠ©æ‰‹å›ç­”

        Args:
            response: åŠ©æ‰‹å›ç­”æ–‡æœ¬
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        ui = ChatUI._ui()
        print()  # é¢å¤–çš„ç©ºè¡Œï¼Œå¢åŠ å¯è¯»æ€§
        ui.panel([response], title=texts.get("response_assistant_title"))
        ui.rule()
        print()  # åœ¨å›ç­”åç•™å‡ºç©ºé—´

    @staticmethod
    def print_full_reasoning(response: "StructuredResponse", texts: I18nTexts):
        """æ˜¾ç¤ºå®Œæ•´çš„æ¨ç†è¿‡ç¨‹

        Args:
            response: ç»“æ„åŒ–å“åº”å¯¹è±¡
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        ui = ChatUI._ui()
        print()
        ui.section_heading(texts.get("response_reasoning_title"))
        ui.panel([texts.get("response_answer_label")])
        ui.panel(response.answer.split("\n"))
        ui.panel([texts.get("response_reasoning_label")])
        ui.panel(response.reasoning.split("\n"))

        confidence_emoji = {"high": "ğŸŸ¢", "medium": "ğŸŸ¡", "low": "ğŸ”´"}
        emoji = confidence_emoji.get(response.confidence, "âšª")
        refs_text = (
            ', '.join(response.references)
            if response.references
            else texts.get("response_no_references")
        )
        meta_lines = [
            f"{emoji} {texts.get('response_confidence')}: {response.confidence.upper()}",
            f"ğŸ“ {texts.get('response_references')}: {refs_text}",
        ]
        ui.panel([texts.get("response_metadata_label")])
        ui.panel(meta_lines)

        if response.additional_notes:
            ui.panel([texts.get("response_notes_label")])
            ui.panel(response.additional_notes.split("\n"))
        print()

    @staticmethod
    def print_help(texts: I18nTexts):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

        Args:
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        ui = ChatUI._ui()
        print()
        ui.section_heading(texts.get("cmd_help_title"))
        lines = [
            f"ğŸšª  {texts.get('cmd_exit')}",
            f"ğŸ§¹  {texts.get('cmd_clear')}",
            f"ğŸ”„  {texts.get('cmd_reload')}",
            f"ğŸ§   {texts.get('cmd_reasoning')}",
            f"â“  {texts.get('cmd_help')}",
        ]
        ui.panel(lines)
        print()

    @staticmethod
    def print_info(message: str, texts: I18nTexts):
        """æ˜¾ç¤ºä¿¡æ¯æç¤º

        Args:
            message: æç¤ºä¿¡æ¯
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        ui = ChatUI._ui()
        print()
        ui.success(f"âœ“ {message}")
        print()

    @staticmethod
    def print_error(message: str, texts: I18nTexts):
        """æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯

        Args:
            message: é”™è¯¯ä¿¡æ¯
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        ui = ChatUI._ui()
        print()
        ui.error(f"âœ— {message}")
        print()


# ============================================================================
# ä¸»å…¥å£å‡½æ•°
# ============================================================================


async def main():
    """ä¸»å…¥å£å‡½æ•° - å¯åŠ¨å¯¹è¯ç³»ç»Ÿ"""

    # 0. é…ç½®æ—¥å¿—çº§åˆ« - éšè—æŠ€æœ¯æ—¥å¿—ï¼Œåªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
    logging.basicConfig(level=logging.WARNING, format='%(levelname)s: %(message)s')
    # ç¡®ä¿æ‰€æœ‰ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—ä¹Ÿéµå¾ªæ­¤è®¾ç½®
    logging.getLogger().setLevel(logging.WARNING)

    # 1. å¯ç”¨è¡Œç¼–è¾‘åŠŸèƒ½
    if READLINE_AVAILABLE:
        # è®¾ç½®å†å²è®°å½•æ–‡ä»¶
        history_file = PROJECT_ROOT / "demo" / ".chat_history"
        try:
            if history_file.exists():
                readline.read_history_file(str(history_file))
            # è®¾ç½®å†å²è®°å½•æœ€å¤§é•¿åº¦
            readline.set_history_length(1000)
        except Exception:
            pass  # å¿½ç•¥å†å²è®°å½•åŠ è½½é”™è¯¯

    # 2. è¯­è¨€é€‰æ‹©
    language = LanguageSelector.select_language()
    texts = I18nTexts(language)

    # 3. æ¸…å±ï¼Œå‡†å¤‡æ˜¾ç¤ºæ¬¢è¿ç•Œé¢
    ChatUI.clear_screen()

    # 4. æ‰“å°æ¬¢è¿æ¨ªå¹…
    ChatUI.print_banner(texts)

    # 5. åœºæ™¯æ¨¡å¼é€‰æ‹©
    scenario_type = ScenarioSelector.select_scenario(texts)
    if not scenario_type:
        ChatUI.print_info(texts.get("groups_not_selected_exit"), texts)
        return

    # 6. åˆ·æ–°å±å¹•ï¼Œä¿ç•™æ¨ªå¹…
    ChatUI.clear_screen()
    ChatUI.print_banner(texts)

    # 7. åˆå§‹åŒ–é…ç½®
    chat_config = ChatModeConfig()
    llm_config = LLMConfig()
    embedding_config = EmbeddingConfig()
    mongo_config = MongoDBConfig()

    # 8. éªŒè¯ LLM API Key
    import os

    api_key_present = any(
        [
            llm_config.api_key,
            os.getenv("OPENROUTER_API_KEY"),
            os.getenv("OPENAI_API_KEY"),
        ]
    )
    if not api_key_present:
        ChatUI.print_error(texts.get("config_api_key_missing"), texts)
        print(f"{texts.get('config_api_key_hint')}\n")
        return

    # 9. åˆå§‹åŒ– MongoDB
    ui = ChatUI._ui()
    ui.note(texts.get("mongodb_connecting"), icon="ğŸ”Œ")
    try:
        await ensure_mongo_beanie_ready(mongo_config)
        print("\r\033[K", end="")  # æ¸…é™¤ "è¿æ¥ MongoDB..." è¡Œ
        print("\033[A\033[K", end="")
    except Exception as e:
        ChatUI.print_error(texts.get("mongodb_init_failed", error=str(e)), texts)
        return

    # 10. ç¾¤ç»„é€‰æ‹©
    groups = await GroupSelector.list_available_groups()
    selected_group_id = await GroupSelector.select_group(groups, texts)

    if not selected_group_id:
        ChatUI.print_info(texts.get("groups_not_selected_exit"), texts)
        return

    # 11. ğŸ”¥ æ£€ç´¢æ¨¡å¼é€‰æ‹©
    retrieval_mode = RetrievalModeSelector.select_retrieval_mode(texts)
    
    if not retrieval_mode:
        ChatUI.print_info(texts.get("groups_not_selected_exit"), texts)
        return

    # 12. åˆ·æ–°å±å¹•ï¼Œä¿ç•™æ¨ªå¹…
    ChatUI.clear_screen()
    ChatUI.print_banner(texts)

    # 13. åˆ›å»ºå¹¶åˆå§‹åŒ–å¯¹è¯ä¼šè¯
    session = ChatSession(
        group_id=selected_group_id,
        config=chat_config,
        llm_config=llm_config,
        embedding_config=embedding_config,
        scenario_type=scenario_type,
        retrieval_mode=retrieval_mode,  # ğŸ”¥ ä¼ é€’æ£€ç´¢æ¨¡å¼
        texts=texts,
    )

    if not await session.initialize():
        ChatUI.print_error(texts.get("session_init_failed"), texts)
        return

    # 14. è¿›å…¥å¯¹è¯å¾ªç¯
    ui = ChatUI._ui()
    print()
    ui.rule()
    ui.note(texts.get("chat_start_note"), icon="ğŸ’¬")
    ui.rule()
    print()

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input(texts.get("chat_input_prompt")).strip()

            if not user_input:
                continue

            # å¤„ç†å‘½ä»¤ï¼ˆå‘½ä»¤ä¸ä¼šè¢«è®°å½•åˆ°å¯¹è¯å†å²ï¼‰
            command = user_input.lower()

            if command == "exit":
                ui = ChatUI._ui()
                print()
                ui.note(texts.get("cmd_exit_saving"), icon="ğŸ’¾")
                await session.save_conversation_history()
                print()
                ui.success(f"âœ“ {texts.get('cmd_exit_complete')}")
                print()
                break

            elif command == "clear":
                session.clear_history()
                continue

            elif command == "reload":
                await session.reload_data()
                continue

            elif command == "reasoning":
                session.show_reasoning()
                continue

            elif command == "help":
                ChatUI.print_help(texts)
                continue

            # æ‰§è¡Œå¯¹è¯ï¼ˆåªæœ‰éå‘½ä»¤çš„è¾“å…¥æ‰ä¼šè¢«è®°å½•åˆ°å†å²ï¼‰
            response = await session.chat(user_input)
            ChatUI.print_assistant_response(response, texts)

        except KeyboardInterrupt:
            ui = ChatUI._ui()
            print("\n")
            ui.note(texts.get("cmd_interrupt_saving"), icon="âš ï¸")
            await session.save_conversation_history()
            print()
            ui.success(f"âœ“ {texts.get('cmd_exit_complete')}")
            print()
            break

        except Exception as e:
            ChatUI.print_error(texts.get("chat_error", error=str(e)), texts)
            import traceback

            traceback.print_exc()
            print()  # é¢å¤–ç©ºè¡Œï¼Œå¢åŠ å¯è¯»æ€§

    # ä¿å­˜è¾“å…¥å†å²
    if READLINE_AVAILABLE:
        try:
            history_file = PROJECT_ROOT / "demo" / ".chat_history"
            readline.write_history_file(str(history_file))
        except Exception:
            pass  # å¿½ç•¥å†å²è®°å½•ä¿å­˜é”™è¯¯


if __name__ == "__main__":
    asyncio.run(main())
