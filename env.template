# =====================================================
# Memory System Configuration Template
# =====================================================
#
# SECURITY NOTICE:
# - Copy this file to .env and fill in your actual API keys
# - Never commit .env file to version control
# - Keep your API keys secure and private
#
# SETUP INSTRUCTIONS:
# 1. cp env.template .env
# 2. Edit .env with your actual values
# 3. The system will automatically load these values
# =====================================================


# ===================
# LLM Configuration
# ===================

LLM_PROVIDER=openai
# For evaluation, use openai/gpt-4.1-mini to ensure reproducibility
# For demo, use x-ai/grok-4-fast for a balance of speed/cost/quality
LLM_MODEL=x-ai/grok-4-fast
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_API_KEY=sk-or-v1-xxxx
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=32768
# OpenRouter/other provider settings, default is "default"
# When using Qwen3 via OpenRouter, consider setting to "cerebras"
# LLM_OPENROUTER_PROVIDER=cerebras

# ===================
# Vectorize (Embedding) Service Configuration
# ===================

# ---- Primary Vectorize Provider ----
# Provider type: vllm (self-deployed), deepinfra (commercial API)
VECTORIZE_PROVIDER=vllm

# API key for primary provider (use "EMPTY" if not required for vllm service)
VECTORIZE_API_KEY=EMPTY

# Base URL for primary provider
# vLLM service example: http://localhost:8000/v1
# DeepInfra example: https://api.deepinfra.com/v1/openai
VECTORIZE_BASE_URL=http://localhost:8000/v1

# Model name (shared by both primary and fallback providers)
VECTORIZE_MODEL=Qwen/Qwen3-Embedding-4B

# ---- Fallback Vectorize Provider (Optional) ----
# Fallback provider type: vllm, deepinfra, or none (to disable fallback)
# Note: Fallback will be disabled if provider is "none", base_url is empty, 
#       or api_key is empty (for deepinfra provider)
VECTORIZE_FALLBACK_PROVIDER=deepinfra

# API key for fallback provider (required for deepinfra, optional for vllm)
VECTORIZE_FALLBACK_API_KEY=xxxxx

# Base URL for fallback provider (required if fallback is enabled)
VECTORIZE_FALLBACK_BASE_URL=https://api.deepinfra.com/v1/openai

# ===== Common Settings =====
VECTORIZE_TIMEOUT=30
VECTORIZE_MAX_RETRIES=3
VECTORIZE_BATCH_SIZE=10
VECTORIZE_MAX_CONCURRENT=5
VECTORIZE_ENCODING_FORMAT=float

# Vector dimensions for client-side truncation
# Set to 0 to disable truncation and use full model dimensions
# Qwen3-Embedding-4B: full 2560D (DeepInfra) or 3584D (vLLM), recommend truncate to 1024D
# Note: Always uses client-side truncation with L2 re-normalization
VECTORIZE_DIMENSIONS=1024


# ===================
# Rerank Service Configuration
# ===================

# ---- Primary Rerank Provider ----
# Provider type: vllm (self-deployed), deepinfra (commercial API)
RERANK_PROVIDER=vllm

# API key for primary provider (use "EMPTY" if not required for vllm service)
RERANK_API_KEY=EMPTY

# Base URL for primary provider
# vLLM service example: http://localhost:12000/v1/rerank
# DeepInfra example: https://api.deepinfra.com/v1/inference
RERANK_BASE_URL=http://localhost:12000/v1/rerank

# Model name (shared by both primary and fallback providers)
RERANK_MODEL=Qwen/Qwen3-Reranker-4B

# ---- Fallback Rerank Provider (Optional) ----
# Fallback provider type: vllm, deepinfra, or none (to disable fallback)
# Note: Fallback will be disabled if provider is "none", base_url is empty,
#       or api_key is empty (for deepinfra provider)
RERANK_FALLBACK_PROVIDER=deepinfra

# API key for fallback provider (required for deepinfra, optional for vllm)
RERANK_FALLBACK_API_KEY=xxxxx

# Base URL for fallback provider (required if fallback is enabled)
RERANK_FALLBACK_BASE_URL=https://api.deepinfra.com/v1/inference

# ===== Common Settings =====
RERANK_TIMEOUT=30
RERANK_MAX_RETRIES=3
RERANK_BATCH_SIZE=10
RERANK_MAX_CONCURRENT=5


# ===================
# Redis Configuration
# ===================

REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=8
REDIS_SSL=false

# ===================
# MongoDB Configuration
# ===================

MONGODB_HOST=localhost
MONGODB_PORT=27017
MONGODB_USERNAME=admin
MONGODB_PASSWORD=memsys123
MONGODB_DATABASE=memsys
MONGODB_URI_PARAMS=socketTimeoutMS=15000&authSource=admin

# ===================
# Elasticsearch Configuration
# ===================

ES_HOSTS=http://localhost:19200
ES_USERNAME=
ES_PASSWORD=
ES_VERIFY_CERTS=false
SELF_ES_INDEX_NS=memsys

# ===================
# Milvus Vector Database Configuration
# ===================

MILVUS_HOST=localhost
MILVUS_PORT=19530
SELF_MILVUS_COLLECTION_NS=memsys

# ===================
# API Server Configuration
# ===================

# V3 API Base URL (used by chat_with_memory.py and other clients)
API_BASE_URL=http://localhost:1995

# ===================
# Environment & Logging
# ===================

LOG_LEVEL=INFO
ENV=dev
PYTHONASYNCIODEBUG=1
MEMORY_LANGUAGE=en
